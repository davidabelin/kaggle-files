{"cells":[{"metadata":{"id":"9n9_cTveKmse","cellView":"both","outputId":"020eac89-769f-4a50-ffde-0f61e7accaaf","trusted":true},"cell_type":"code","source":"import random as rd\nimport time\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom matplotlib import pyplot as plt\n\n#submit = True\nsubmit = False","execution_count":null,"outputs":[]},{"metadata":{"id":"JZlvdpyYKx7V","cellView":"both","outputId":"ebec3716-5b62-4a1a-fae5-1cd75c19b201","trusted":true},"cell_type":"code","source":"# load the public MNIST dataset.\n(x_train, y_train),(x_test, y_test) = tf.keras.datasets.mnist.load_data()\n\nx_train_norm = x_train/255.\nx_test_norm = x_test/255.","execution_count":null,"outputs":[]},{"metadata":{"id":"1aGtML-Okikx","trusted":true},"cell_type":"code","source":"kaggle = pd.read_csv('../input/digit-recognizer/test.csv')\nkaggle_norm = np.asarray(kaggle/255.)\n\nk_train = pd.read_csv('../input/digit-recognizer/train.csv')\nk_labels = np.asarray(k_train['label'])\nk_train = k_train.drop(columns=['label'])\nk_train_norm = np.asarray(k_train/255.)","execution_count":null,"outputs":[]},{"metadata":{"id":"_CYqd8LMZ75Z","trusted":true},"cell_type":"code","source":"# Add a channels dimension\nk_train_norm = k_train_norm.reshape(42000, 28, 28, 1)\nkaggle_norm = kaggle_norm.reshape(28000, 28, 28, 1)\nx_train_norm = x_train_norm.reshape(60000, 28, 28, 1)\nx_test_norm = x_test_norm.reshape(10000, 28, 28, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train on the training set with some held back for validation #\ndef train_model(model, train_features, train_label, epochs,\n                batch_size=None, validation_split=None):\n\n    history = model.fit(x=train_features, y=train_label, \n                        batch_size=batch_size,\n                        epochs=epochs, shuffle=True, \n                        validation_split=validation_split,\n                        verbose = 1)\n\n    # Gather the model's metrics after each round of training\n    epochs = history.epoch\n    hist = pd.DataFrame(history.history)\n    return epochs, hist","execution_count":null,"outputs":[]},{"metadata":{"id":"1C_tbFXcMHu7","trusted":true},"cell_type":"code","source":" # Set up a CNN with Keras\n \ndef create_CNN(learning_rate):\n    \"\"\"Create and compile a convolutional neural net.\"\"\"  \n    # Define the kind of model to use.\n    model = tf.keras.models.Sequential()\n    model.add(tf.keras.layers.Conv2D(32, 6, activation='relu', input_shape=(28, 28, 1)))\n    model.add(tf.keras.layers.Conv2D(64, 4, activation='relu'))\n    model.add(tf.keras.layers.Conv2D(128, 2, activation='relu'))\n    model.add(tf.keras.layers.MaxPooling2D((2,2)))\n    model.add(tf.keras.layers.Flatten())\n    model.add(tf.keras.layers.Dense(units=100, activation='softmax'))     \n    model.compile(optimizer=tf.keras.optimizers.Adam(lr=learning_rate),\n                    loss=\"sparse_categorical_crossentropy\",\n                    metrics=['accuracy']) \n    return model","execution_count":null,"outputs":[]},{"metadata":{"cellView":"both","id":"Nn2eBWKXTuaX","outputId":"aa37d1d2-d5f8-453b-c2a8-053549749d34","trusted":true,"collapsed":true},"cell_type":"code","source":"# Train and evalate CNN on ALL the datasets, \n# Train on 10000 + 60000 + 42000 to predict on 28000\nlearning_rate = 0.001\nepochs = 5\nbatch_size = 500\nvalidation_split = 0.005 # 0.5% off bottom of each array\nstart_time = time.time()\n\nconvoluter = create_CNN(learning_rate)\n \n# TRAIN X8:\nfor _ in range(8): #40 epochs total on each dataset\n    #x_test_norm:\n    epochs_CNN1, hist_CNN1 = train_model(convoluter, x_test_norm, y_test, epochs, batch_size, validation_split)\n    print(\"\\nFinished test set of 10000 in about {} seconds\\n\".format((round(time.time()-start_time,4))))\n    batch_time = time.time()\n    #x_train_norm:\n    epochs_CNN2, hist_CNN2 = train_model(convoluter, x_train_norm, y_train, epochs, batch_size, validation_split)\n    print(\"\\nFinished test set of 60000 in about {} seconds\\n\".format((round(time.time()-start_time,4))))\n    batch_time = time.time()\n    #k_train_norm:\n    epochs_CNN3, hist_CNN3 = train_model(convoluter, k_train_norm, k_labels, epochs, batch_size, validation_split)\n    print(\"\\nfinished test set of 42000 in about {} seconds\\n\".format((round(time.time()-start_time,4))))\n\nprint (\"\\nTotal time: {} seconds is about {} minutes.\".format(round(time.time()-start_time,4),\n                                                              (time.time()-start_time)//60))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if submit:\n    \n    def getKaggles():  \n        kaggles = pd.DataFrame(columns=['ImageId','Label'])  \n        predicts = convoluter.predict(kaggle_norm)\n        for j in range(len(kaggle_norm)):\n            probs = predicts[j] # one row of 10 probabilities \n            max_id = np.argmax(probs)   # index of top probability in row\n            kaggles.at[j,'ImageId'] = j+1\n            kaggles.at[j,'Label'] = max_id\n        return kaggles\n    \n    kaggles = getKaggles()\n    kaggles.to_csv('submission.csv', columns=[\"ImageId\",\"Label\"], index=False)","execution_count":null,"outputs":[]},{"metadata":{"id":"9H7W-OKHUnsp"},"cell_type":"markdown","source":"##Visualizations"},{"metadata":{"id":"N6QCoQqRvRfb","outputId":"024760b8-e478-43d5-cc84-4621159a5776","trusted":true,"collapsed":true},"cell_type":"code","source":"if not submit:\n# Plot a graph of the 'accuracy' metric vs. epochs:\n    plt.plot(epochs_CNN3,hist_CNN3[\"accuracy\"])\n    plt.plot(epochs_CNN3,hist_CNN3[\"val_accuracy\"])","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}