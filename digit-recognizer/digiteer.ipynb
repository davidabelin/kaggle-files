{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "digiteer.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "JndnmDMp66FL",
        "266KQvZoMxMv",
        "6sfw3LH0Oycm"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDlWLbfkJtvu",
        "cellView": "form"
      },
      "source": [
        "#@title Everything not mine is copyright 2020 Google LLC. Double-click here for full information.\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        " \n",
        "# Yann LeCun and Corinna Cortes hold the copyright of MNIST dataset,\n",
        "# which is a derivative work from original NIST datasets. \n",
        "# MNIST dataset is made available under the terms of the \n",
        "# Creative Commons Attribution-Share Alike 3.0 license."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SBux8qWSzPz"
      },
      "source": [
        "This Notebook is heavily modified from the MLCC programming project with single-digit images here:\n",
        "https://colab.research.google.com/github/google/eng-edu/blob/master/ml/cc/exercises/multi-class_classification_with_MNIST.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9n9_cTveKmse",
        "cellView": "both"
      },
      "source": [
        "# load some standard utilities.\n",
        "%tensorflow_version 2.x\n",
        "#from __future__ import absolute_import, division, print_function, unicode_literals\n",
        " \n",
        "import random as rd\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from matplotlib import pyplot as plt\n",
        " \n",
        "# The following lines adjust the granularity of reporting. \n",
        "pd.options.display.max_rows = 10\n",
        "pd.options.display.float_format = \"{:.3f}\".format\n",
        " \n",
        "# The following line improves formatting when ouputting NumPy arrays.\n",
        "np.set_printoptions(linewidth = 200)\n",
        " \n",
        "print(\"Loaded modules.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZlvdpyYKx7V",
        "cellView": "both"
      },
      "source": [
        "# load the dataset.\n",
        "#(x_train, y_train),(x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "#print(\"Loaded the train and test sets.\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQrwGrKCL_8P"
      },
      "source": [
        "x_train_norm = x_train/255.\n",
        "x_test_norm = x_test/255."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3014ezH3C7jT"
      },
      "source": [
        "## Create a deep neural net model and a convolutional neural network to compare"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5AI0Drej0KS"
      },
      "source": [
        "# SET UP A DEEP NEURAL NET \n",
        " \n",
        "def create_DNN(learning_rate):\n",
        "    \"\"\"Create and compile a deep neural net.\"\"\"  \n",
        "    # Define the kind of model to use.\n",
        "    model = tf.keras.models.Sequential()\n",
        "    model.add(tf.keras.layers.Flatten(input_shape=(28, 28)))\n",
        "    model.add(tf.keras.layers.Dense(units=128, activation='relu'))\n",
        "    model.add(tf.keras.layers.Dropout(rate=0.1)) #avoid overfitting to train set\n",
        "    model.add(tf.keras.layers.Dense(units=256, activation='relu'))\n",
        "    model.add(tf.keras.layers.Dropout(rate=0.2))\n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "    model.add(tf.keras.layers.Dense(units=100, activation='softmax'))     \n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(lr=learning_rate),\n",
        "                    loss=\"sparse_categorical_crossentropy\",\n",
        "                    metrics=['accuracy']) \n",
        "    return model "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1C_tbFXcMHu7"
      },
      "source": [
        " # SET UP A **CONVOLUTIONAL** NEURAL NET \n",
        " \n",
        "def create_CNN(learning_rate):\n",
        "    \"\"\"Create and compile a convolutional neural net.\"\"\"  \n",
        "    # Define the kind of model to use.\n",
        "    model = tf.keras.models.Sequential()\n",
        "    model.add(tf.keras.layers.Conv2D(32, 6, activation='relu', input_shape=(28, 28, 1)))\n",
        "    model.add(tf.keras.layers.Conv2D(64, 4, activation='relu'))\n",
        "    model.add(tf.keras.layers.Conv2D(128, 2, activation='relu'))\n",
        "    model.add(tf.keras.layers.MaxPooling2D((2,2)))\n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "    model.add(tf.keras.layers.Dense(units=100, activation='softmax'))     \n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(lr=learning_rate),\n",
        "                    loss=\"sparse_categorical_crossentropy\",\n",
        "                    metrics=['accuracy']) \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vv2HcemXd0fB"
      },
      "source": [
        "x_train_norm.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQAjVbCtM61V"
      },
      "source": [
        " # train on the training set with 10% held back for validation #\n",
        "def train_model(model, train_features, train_label, epochs,\n",
        "                batch_size=None, validation_split=0.1):\n",
        "\n",
        "    history = model.fit(x=train_features, y=train_label, \n",
        "                        batch_size=batch_size,\n",
        "                        epochs=epochs, shuffle=True, \n",
        "                        validation_split=validation_split)\n",
        "\n",
        "    # Gather the model's metrics after each round of training\n",
        "    epochs = history.epoch\n",
        "    hist = pd.DataFrame(history.history)\n",
        "    return epochs, hist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxgw5eyKfyft"
      },
      "source": [
        "x_train_norm = x_train_norm.reshape(60000, 28, 28)\n",
        "x_test_norm = x_test_norm.reshape(10000, 28, 28)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nj3v5EKQFY8s",
        "cellView": "both"
      },
      "source": [
        "# The adjustable \"hyperparameters\" for the model\n",
        "learning_rate = 0.001\n",
        "epochs = 20\n",
        "batch_size = 500\n",
        "validation_split = 0.1\n",
        " \n",
        "dense_NN = create_DNN(learning_rate)\n",
        " \n",
        "# TRAIN:\n",
        "epochs_DNN, hist_DNN = train_model(dense_NN, x_train_norm, y_train, \n",
        "                                    epochs, batch_size, validation_split)\n",
        " \n",
        "# EVALUATE AGAINST THE TEST SET:\n",
        "print(\"\\n Evaluate the new model against the test set:\")\n",
        "dense_NN.evaluate(x=x_test_norm, \n",
        "                  y=y_test, \n",
        "                  batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CYqd8LMZ75Z"
      },
      "source": [
        "x_train_norm = x_train_norm.reshape(60000, 28, 28, 1)\n",
        "x_test_norm = x_test_norm.reshape(10000, 28, 28, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "id": "Nn2eBWKXTuaX"
      },
      "source": [
        "# Train and evalate CNN\n",
        " \n",
        "convolute_NN = create_CNN(learning_rate)\n",
        " \n",
        "# TRAIN:\n",
        "epochs_CNN, hist_CNN = train_model(convolute_NN, x_train_norm, y_train, epochs, batch_size, validation_split)\n",
        " \n",
        "# EVALUATE AGAINST THE TEST SET:\n",
        "print(\"\\n Evaluate the new model against the test set:\")\n",
        "convolute_NN.evaluate(x=x_test_norm, \n",
        "                        y=y_test, \n",
        "                        batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9H7W-OKHUnsp"
      },
      "source": [
        "##Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6QCoQqRvRfb"
      },
      "source": [
        "# Plot a graph of the 'accuracy' metric vs. epochs:\n",
        "#plot_curve(epochs, hist, 'accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKFOumI6vg4T"
      },
      "source": [
        "def getAnswers(how_many=10):  \n",
        "    answers = pd.DataFrame(columns=['Answer','Guess','P(A)','P(G)'])  \n",
        "    answers['Answer'] = y_test[:how_many]\n",
        "    predicts = my_model.predict(x_test_norm).round(5)\n",
        "    for j in range(0,how_many): # how_many is the number of examples to guess\n",
        "        probs = predicts[j] # one row of 100 probabilities \n",
        "        maxr = max(probs)   # top probability\n",
        "        for i in range(0,100):\n",
        "            if probs[i] == maxr:\n",
        "                answers.at[j,'Guess'] = i\n",
        "                answers.at[j,'P(G)'] = maxr*100\n",
        "            if i == answers['Answer'][j]:\n",
        "                answers.at[j,'P(A)'] = probs[i]*100\n",
        "    return answers\n",
        "print(\"Loaded function getAnswers.\")\n",
        "print(\"Getting answers...\" )\n",
        "\n",
        "# LOAD UP ALL THE GUESSES (W/ PROBABILITES) FOR \n",
        "# EACH EXAMPLE IMAGE IN THE NORMALIZED TEST SET\n",
        "answers = getAnswers(len(x_test_norm))\n",
        "answers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkl_yxX8GlEt"
      },
      "source": [
        "# FIND THE *WRONG* GUESSES\n",
        "wrongs = pd.DataFrame(index=answers.index,columns=['Mask'])\n",
        "for row in answers.index:\n",
        "    if answers['Answer'][row] == answers['Guess'][row]:\n",
        "        wrongs['Mask'][row] = False\n",
        "    else:\n",
        "        wrongs['Mask'][row] = True\n",
        "\n",
        "# LOCATE AND PRINT OUT THE LIST OF WRONG GUESSES      \n",
        "ans = answers.loc[wrongs['Mask']]\n",
        "w = len(ans) \n",
        "t = len(x_test_norm)\n",
        "print(w,\"wrong out of\",t,\"guesses.\")\n",
        "ans"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZ4riZ5VmQbV"
      },
      "source": [
        "ww = []\n",
        "for i in range(0,100):\n",
        "    w = len((answers.loc[lambda a: a['Answer'] == i]).loc[wrongs['Mask']])\n",
        "    ww += [w]\n",
        "attempts = pd.DataFrame(ww)\n",
        "attempts.sort_values(0).tail(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjUfVVBbPq2v"
      },
      "source": [
        "g = answers['Guess'].loc[wrongs['Mask']]\n",
        "a = answers['Answer'].loc[wrongs['Mask']]\n",
        "x = rd.choice(g.index)  #randomly select a *wrong* guess\n",
        "pa = round(answers['P(A)'][x],2)\n",
        "pg = round(answers['P(G)'][x],2)\n",
        "print(\"Answer[\",x,\"] is\",a[x],\"with estimated probability\",pa,\"\\n\")\n",
        "print(\"Guess[\",x,\"] was\",g[x],\"with estimated probability\",pg,\"\\n\")\n",
        "plt.imshow(x_test_norm[x])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddyDxHDGPq2q"
      },
      "source": [
        "# find the digits that are the most difficult for the NN\n",
        "ww = []\n",
        "for i in range(0,100):\n",
        "    w = len((answers.loc[lambda a: a['Answer'] == i]).loc[wrongs['Mask']])\n",
        "    ww += [w]\n",
        "attempts = pd.DataFrame(ww)\n",
        "attempts.sort_values(0).tail(5)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}