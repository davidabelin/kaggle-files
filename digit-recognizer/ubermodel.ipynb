{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ubermodel.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOR9HGAvP5jj"
      },
      "source": [
        "#Ubermodel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "id": "9n9_cTveKmse"
      },
      "source": [
        "import random as rd\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "#submit = True\n",
        "submit = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "id": "JZlvdpyYKx7V"
      },
      "source": [
        "# Load data\n",
        "\n",
        "#From Kaggle competition:\n",
        "#k_train = pd.read_csv('../input/digit-recognizer/train.csv')\n",
        "#k_train = pd.read_csv('../content/train.csv')\n",
        "k_train = pd.read_csv('https://github.com/davidabelin/kaggle-files/raw/main/digit-recognizer/train.csv')\n",
        "kaggle_data = [list(k_train.iloc[idx]) for idx in k_train.index]\n",
        "\n",
        "# the public MNIST dataset.\n",
        "(x_train, y_train),(x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "x_test = x_test.reshape((10000, 784))\n",
        "x_train = x_train.reshape((60000, 784))\n",
        "\n",
        "dummy = []\n",
        "for i in range(len(x_test)):\n",
        "    line = list(x_test[i])\n",
        "    line.insert(0, int(y_test[i]))\n",
        "    dummy += line\n",
        "for i in range(len(x_train)):  \n",
        "    line = list(x_train[i])\n",
        "    line.insert(0, int(y_train[i]))\n",
        "    dummy += line  \n",
        "mnist_data = np.asarray(dummy).reshape((70000,785))\n",
        "mnist_data = [list(row) for row in mnist_data]\n",
        "\n",
        "all_data = mnist_data + kaggle_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cy9BzUKdVVpB"
      },
      "source": [
        "all_data = np.random.permutation(all_data)\n",
        "all_data = np.asarray(all_data)\n",
        "data_df = pd.DataFrame(all_data)\n",
        "\n",
        "data_0_norm, labels_0 = np.asarray(data_df.loc[0:19999, 1:]/255.), np.array(data_df.loc[0:19999,0])\n",
        "data_1_norm, labels_1 = np.asarray(data_df.loc[20000:39999, 1:]/255.), np.array(data_df.loc[20000:39999,0])\n",
        "data_2_norm, labels_2 = np.asarray(data_df.loc[40000:59999, 1:]/255.), np.array(data_df.loc[40000:59999,0])\n",
        "train_data_norm, train_labels = np.asarray(data_df.loc[:59999, 1:]/255.), np.array(data_df.loc[:59999,0])\n",
        "val_data_norm, val_labels = np.asarray(data_df.loc[60000:61999:, 1:]/255.), np.array(data_df.loc[60000:61999,0])\n",
        "uber_data_norm, uber_labels = np.asarray(data_df.loc[62000:, 1:]/255.), np.array(data_df.loc[62000:,0])\n",
        "\n",
        "data_0_norm = data_0_norm.reshape(20000, 28, 28, 1)\n",
        "data_1_norm = data_1_norm.reshape(20000, 28, 28, 1)\n",
        "data_2_norm = data_2_norm.reshape(20000, 28, 28, 1)\n",
        "train_data_norm = train_data_norm.reshape(60000, 28, 28, 1)\n",
        "val_data_norm = val_data_norm.reshape(2000, 28, 28, 1)\n",
        "uber_data_norm = uber_data_norm.reshape(50000, 28, 28, 1)\n",
        "\n",
        "\n",
        "#### TO DO: separate by digit -- train submodels on 2 or 3 digits each"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBHU76baSzlJ"
      },
      "source": [
        "# train on the training set with some held back for validation #\n",
        "def train_model(model, train_features, train_label, epochs,\n",
        "                batch_size=None, validation_split=None, verbose=0):\n",
        "\n",
        "    history = model.fit(x=train_features, y=train_label, \n",
        "                        batch_size=batch_size,\n",
        "                        epochs=epochs, shuffle=True, \n",
        "                        validation_split=validation_split,\n",
        "                        verbose = verbose)\n",
        "\n",
        "    # Gather the model's metrics after each round of training\n",
        "    epochs = history.epoch\n",
        "    hist = pd.DataFrame(history.history)\n",
        "    return epochs, hist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1C_tbFXcMHu7"
      },
      "source": [
        " # Set up a CNN with Keras\n",
        " \n",
        "def create_CNN(learning_rate):\n",
        "    \"\"\"Create and compile a convolutional neural net.\"\"\"  \n",
        "    # Define the kind of model to use.\n",
        "    model = tf.keras.models.Sequential()\n",
        "    model.add(tf.keras.layers.Conv2D(32, 5, activation='relu', input_shape=(28, 28, 1)))\n",
        "    model.add(tf.keras.layers.MaxPooling2D((2,2)))\n",
        "    model.add(tf.keras.layers.Conv2D(64, 3, activation='relu'))\n",
        "    model.add(tf.keras.layers.MaxPooling2D((2,2)))\n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "    model.add(tf.keras.layers.Dense(units=10, activation='softmax'))     \n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(lr=learning_rate),\n",
        "                    loss=\"sparse_categorical_crossentropy\",\n",
        "                    metrics=['accuracy']) \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ViVafL--X5bG"
      },
      "source": [
        "def create_mixedNN(learning_rate):\n",
        "    \"\"\"Create and compile a deep neural net.\"\"\"  \n",
        "    # Define the kind of model to use.\n",
        "    model = tf.keras.models.Sequential()\n",
        "    model.add(tf.keras.layers.Conv2D(32, 6, activation='relu', input_shape=(28, 28, 1)))\n",
        "    model.add(tf.keras.layers.Conv2D(64, 2, activation='relu'))\n",
        "    model.add(tf.keras.layers.MaxPooling2D((2,2)))\n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "    model.add(tf.keras.layers.Dense(units=1024, activation='relu'))\n",
        "    model.add(tf.keras.layers.Dropout(0.3))\n",
        "    model.add(tf.keras.layers.Dense(units=512, activation='relu'))\n",
        "    model.add(tf.keras.layers.Dropout(0.2))\n",
        "    model.add(tf.keras.layers.Dense(units=10, activation='softmax'))     \n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(lr=learning_rate),\n",
        "                    loss=\"sparse_categorical_crossentropy\",\n",
        "                    metrics=['accuracy']) \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwA2HYpe1fdW"
      },
      "source": [
        "def getModel(learning_rate):\n",
        "    inputs = keras.Input(shape=(28, 28, 1))\n",
        "    nn1 = layers.Conv2D(32, 6, activation='relu')(inputs)\n",
        "    nn1 = layers.Conv2D(64, 4, activation='relu')(nn1)\n",
        "    nn1 = layers.MaxPooling2D((2,2))(nn1)\n",
        "\n",
        "    nn2 = layers.Conv2D(32, 6, activation='relu')(inputs)\n",
        "    nn2 = layers.Conv2D(64, 4, activation='relu')(nn2)\n",
        "    nn2 = layers.AveragePooling2D((2,2))(nn2)\n",
        "    \n",
        "    nn = layers.Concatenate()([nn1,nn2])\n",
        "    nn = layers.Conv2D(128, (2,3), activation='relu')(nn)\n",
        "    nn = layers.MaxPooling2D((2,2))(nn)\n",
        "    nn = layers.Flatten()(nn)\n",
        "    nn = layers.Dense(units=256, activation='relu')(nn)\n",
        "    nn = layers.Dropout(0.3)(nn)\n",
        "    nn = layers.Dense(units=128, activation='relu')(nn)\n",
        "    nn = layers.Dropout(0.2)(nn)\n",
        "    \n",
        "    outputs = layers.Dense(10, activation=\"softmax\")(nn)\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    model.compile(optimizer=keras.optimizers.Adam(lr=learning_rate),\n",
        "                    loss=\"sparse_categorical_crossentropy\",\n",
        "                    metrics=['accuracy']) \n",
        "    return model    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMfitb4zTuiS"
      },
      "source": [
        "def getDNNModel(learning_rate):\n",
        "    inputs = keras.Input(shape=(28, 28, 1))\n",
        "    dnn = layers.Flatten()(inputs)\n",
        "    dnn = layers.Dense(768, activation='relu')(dnn)\n",
        "    dnn = layers.Dropout(0.3)(dnn)\n",
        "    dnn = layers.Dense(units=512, activation='relu')(dnn)\n",
        "    dnn = layers.Dropout(0.2)(dnn)\n",
        "    dnn = layers.Dense(units=256, activation='relu')(dnn)\n",
        "    dnn = layers.Dropout(0.1)(dnn)\n",
        "    outputs = layers.Dense(10, activation=\"softmax\")(dnn)\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    model.compile(optimizer=keras.optimizers.Adam(lr=learning_rate),\n",
        "                    loss=\"sparse_categorical_crossentropy\",\n",
        "                    metrics=['accuracy']) \n",
        "    return model    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Kz7CP4jaGHB"
      },
      "source": [
        "learning_rate = 0.001\n",
        "epochs = 30\n",
        "batch_size = 500\n",
        "\n",
        "if submit:\n",
        "    validation_split = None\n",
        "    verbose = 0\n",
        "else:\n",
        "    validation_split = 0.05 # off bottom\n",
        "    verbose = 1\n",
        "\n",
        "model_A = create_CNN(learning_rate)\n",
        "model_B = getModel(learning_rate)\n",
        "model_C = create_mixedNN(learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5G4pl-a47zVK"
      },
      "source": [
        "#model_B.summary()\r\n",
        "keras.utils.plot_model(model_B, \"model_B.png\", show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "id": "Nn2eBWKXTuaX"
      },
      "source": [
        "#x_test_norm:\n",
        "start_time = time.time()\n",
        "epochs_A, hist_A = train_model(model_A, train_data_norm, train_labels, epochs, batch_size, validation_split, verbose=verbose)\n",
        "if not submit:\n",
        "    print(\"\\nTrained model A in about {:.3f} seconds\\n\".format(time.time()-start_time))\n",
        "batch_time = time.time()\n",
        "\n",
        "#x_train_norm:\n",
        "epochs_B, hist_B = train_model(model_B, train_data_norm, train_labels, epochs, batch_size, validation_split, verbose=verbose)\n",
        "if not submit:\n",
        "    print(\"\\nTrained model B in about {:.3f} seconds\\n\".format(time.time()-batch_time))\n",
        "batch_time = time.time()\n",
        "\n",
        "#k_train_norm:\n",
        "epochs_C, hist_C = train_model(model_C, train_data_norm, train_labels, epochs, batch_size, validation_split, verbose=verbose)\n",
        "if not submit:\n",
        "    print(\"\\nTrained model C in about {:.3f} seconds\\n\".format(time.time()-batch_time))\n",
        "\n",
        "if not submit:\n",
        "    print (\"\\nTotal time: {:.3f} seconds is about {:.1f} minutes.\".format(time.time()-start_time,\n",
        "                                                                    (time.time()-start_time)//60))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XaLUPOicU9x"
      },
      "source": [
        "if not submit:\n",
        "    print (\"Evaluate against val_data_norm:\")\n",
        "    print (\"A:\",model_A.evaluate(val_data_norm, val_labels, batch_size=100))\n",
        "    print (\"B:\",model_B.evaluate(val_data_norm, val_labels, batch_size=100))\n",
        "    print (\"C:\",model_C.evaluate(val_data_norm, val_labels, batch_size=100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZOGd10D0NkF"
      },
      "source": [
        "model_B.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQoJ-mA6SpPF"
      },
      "source": [
        "def getUberModel(learning_rate, modelA, modelB, modelC):\n",
        "    \n",
        "    inputs = keras.Input(shape=(28, 28, 1))\n",
        "    xA = modelA(inputs)\n",
        "    xB = modelB(inputs)\n",
        "    xC = modelC(inputs)\n",
        "    x = layers.concatenate([xA, xB, xC])\n",
        "    x = layers.Dense(300, activation='relu')(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    outputs = layers.Dense(10, activation=\"softmax\")(x)\n",
        "    ubermodel = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    ubermodel.compile(optimizer=keras.optimizers.Adam(lr=learning_rate),\n",
        "                            loss=\"sparse_categorical_crossentropy\",\n",
        "                            metrics=['accuracy']) \n",
        "    return ubermodel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-GQdo-oel-f"
      },
      "source": [
        "ubermodel = getUberModel(learning_rate, model_A, model_B, model_C)\n",
        "#ubermodel.summary()\n",
        "keras.utils.plot_model(ubermodel, \"ubermodel.png\", show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RuhvEOaMP5jp"
      },
      "source": [
        "#Ubermodel train:\n",
        "start_time = time.time()\n",
        "epochs_U, hist_U = train_model(ubermodel, uber_data_norm, uber_labels, epochs+20, batch_size, validation_split, verbose=verbose)\n",
        "\n",
        "if not submit:\n",
        "    print(\"\\nTrained model C in about {} seconds\\n\".format((round(time.time()-start_time,4))))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-NUejiaykrD"
      },
      "source": [
        "print (\"Ubermodel:\",ubermodel.evaluate(val_data_norm, val_labels, batch_size=100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBAx2jir6P6v"
      },
      "source": [
        "digiteer = keras.models.load_model(filepath='/content/digiteer_model.zip')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5bHGejdaxby"
      },
      "source": [
        "def getPredictions(model=None, dataset=None): \n",
        "    ''' Takes a trained model and data\n",
        "        Returns a List of guesses for that model on that data\n",
        "    ''' \n",
        "    predictions = []\n",
        "    predicts = model.predict(dataset)\n",
        "    for j in range(len(dataset)):\n",
        "        probs = predicts[j] # one row of 10 probabilities \n",
        "        max_id = np.argmax(probs)   # index of top probability in row\n",
        "        predictions += [max_id]\n",
        "    return predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzxZVtm0P5jp"
      },
      "source": [
        "def getProbabilities(model=None, dataset=None):\n",
        "    ''' Takes a trained model and a set of data\n",
        "        Returns a List of guesses by that model for that data\n",
        "    ''' \n",
        "    return model.predict(dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aGtML-Okikx"
      },
      "source": [
        "#kaggle = pd.read_csv('../input/digit-recognizer/test.csv')\n",
        "kaggle = pd.read_csv('https://github.com/davidabelin/kaggle-files/raw/main/digit-recognizer/test.csv')\n",
        "kaggle_norm = np.asarray(kaggle/255.)\n",
        "kaggle_norm = kaggle_norm.reshape(28000, 28, 28, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QBcAX5baRij"
      },
      "source": [
        "kag_guesses = pd.DataFrame(columns=['A', 'B', 'C', 'AB','AC','CB'])\n",
        "\n",
        "kag_guesses['A'] = getPredictions(model=model3D_A, dataset=kaggle_norm)\n",
        "kag_guesses['B'] = getPredictions(model=model3D_B, dataset=kaggle_norm)\n",
        "kag_guesses['C'] = getPredictions(model=model3D_C, dataset=kaggle_norm)\n",
        "\n",
        "for i in range(len(kag_guesses['A'])):\n",
        "    kag_guesses['AB'] = kag_guesses['A'] - kag_guesses['B']\n",
        "    kag_guesses['AC'] = kag_guesses['A'] - kag_guesses['C']\n",
        "    kag_guesses['CB'] = kag_guesses['C'] - kag_guesses['B']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9J2IS3MhP5jq"
      },
      "source": [
        "kag_probs = pd.DataFrame(columns=list(range(30)))\n",
        "\n",
        "getProbabilities()\n",
        "uberguesses = getPredictions(model=ubermodel, )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UUac0XGP5jq"
      },
      "source": [
        "if not submit:\n",
        "#All agree on them all!?\n",
        "    kag_guesses['CB'].sum() + kag_guesses['AC'].sum() + kag_guesses['AB'].sum()\n",
        "    kag_guesses.to_csv('guessesABC.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8mmj0gsxMD0"
      },
      "source": [
        "if submit:\n",
        "    kaggles = pd.DataFrame(columns=['ImageId','Label']) \n",
        "    kaggles['Label'] = kag_guesses['C']\n",
        "    kaggles['ImageId'] = [i+1 for i in kaggles.index.values] \n",
        "    kaggles.to_csv('submission.csv', columns=[\"ImageId\",\"Label\"], index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9H7W-OKHUnsp"
      },
      "source": [
        "##Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSy9WCq7P5jq"
      },
      "source": [
        "k_img = np.asarray(kaggle.iloc[20044 - 1])\n",
        "k_img = k_img.reshape(28,28)\n",
        "plt.imshow(k_img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6QCoQqRvRfb"
      },
      "source": [
        "if not submit:\n",
        "# Plot a graph of the 'accuracy' metric vs. epochs:\n",
        "    plt.plot(epochs_A,hist_A[\"accuracy\"])\n",
        "    plt.plot(epochs_A,hist_A[\"val_accuracy\"])\n",
        "    plt.show()\n",
        "\n",
        "    plt.plot(epochs_B,hist_B[\"accuracy\"])\n",
        "    plt.plot(epochs_B,hist_B[\"val_accuracy\"])\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}