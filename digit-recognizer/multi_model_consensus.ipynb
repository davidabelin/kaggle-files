{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "multi-model-consensus.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOR9HGAvP5jj"
      },
      "source": [
        "#Maybe overkill..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "id": "9n9_cTveKmse"
      },
      "source": [
        "import random as rd\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "#submit = True\n",
        "submit = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "id": "JZlvdpyYKx7V"
      },
      "source": [
        "# load the public MNIST dataset.\n",
        "(x_train, y_train),(x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "x_train = x_train.reshape(60000,784)\n",
        "x_train_norm = x_train/255.\n",
        "\n",
        "x_test = x_test.reshape(10000,784)\n",
        "x_test_norm = x_test/255.\n",
        "x_test_norm = x_test.reshape((10000, 28, 28, 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ubww4azcuuHr"
      },
      "source": [
        "#k_train = pd.read_csv('../input/digit-recognizer/train.csv')\n",
        "#k_labels = np.asarray(k_train['label'])\n",
        "#k_train = k_train.drop(columns=['label'])\n",
        "#k_train_norm = np.asarray(k_train/255.)\n",
        "\n",
        "#val_set_labels = k_labels[-2000:]\n",
        "#val_set_norm = k_train_norm[-2000:]\n",
        "#val_set_norm = val_set_norm.reshape(2000, 28, 28, 1)\n",
        "\n",
        "#k_train_norm = k_train_norm[:-2000]\n",
        "#k_labels = k_labels[:-2000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cO828kSsxzC"
      },
      "source": [
        "#x_data = [row for row in x_test_norm]\n",
        "x_data = [row for row in x_train_norm]\n",
        "#x_data += [row for row in k_train_norm]\n",
        "x_data = np.asarray(x_data)\n",
        "x_data = x_data.reshape((60000, 28, 28, 1))\n",
        "\n",
        "#y_data = [label for label in y_test]\n",
        "y_data = [label for label in y_train]\n",
        "#y_data += [label for label in k_labels]\n",
        "y_data = np.asarray(y_data).flatten()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBHU76baSzlJ"
      },
      "source": [
        "# train on the training set with some held back for validation #\n",
        "def train_model(model, train_features, train_label, epochs,\n",
        "                batch_size=None, validation_split=None, verbose=0):\n",
        "\n",
        "    history = model.fit(x=train_features, y=train_label, \n",
        "                        batch_size=batch_size,\n",
        "                        epochs=epochs, shuffle=True, \n",
        "                        validation_split=validation_split,\n",
        "                        verbose = verbose)\n",
        "\n",
        "    # Gather the model's metrics after each round of training\n",
        "    epochs = history.epoch\n",
        "    hist = pd.DataFrame(history.history)\n",
        "    return epochs, hist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1C_tbFXcMHu7"
      },
      "source": [
        " # Set up a CNN with Keras\n",
        " \n",
        "def create_CNN(learning_rate):\n",
        "    \"\"\"Create and compile a convolutional neural net.\"\"\"  \n",
        "    # Define the kind of model to use.\n",
        "    model = tf.keras.models.Sequential()\n",
        "    model.add(tf.keras.layers.Conv2D(32, 5, activation='relu', input_shape=(28, 28, 1)))\n",
        "    model.add(tf.keras.layers.MaxPooling2D((2,2)))\n",
        "    model.add(tf.keras.layers.Conv2D(64, 3, activation='relu'))\n",
        "    model.add(tf.keras.layers.MaxPooling2D((2,2)))\n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "    model.add(tf.keras.layers.Dense(units=10, activation='softmax'))     \n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(lr=learning_rate),\n",
        "                    loss=\"sparse_categorical_crossentropy\",\n",
        "                    metrics=['accuracy']) \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ViVafL--X5bG"
      },
      "source": [
        "def create_mixedNN(learning_rate):\n",
        "    \"\"\"Create and compile a deep neural net.\"\"\"  \n",
        "    # Define the kind of model to use.\n",
        "    model = tf.keras.models.Sequential()\n",
        "    model.add(tf.keras.layers.Conv2D(32, 6, activation='relu', input_shape=(28, 28, 1)))\n",
        "    model.add(tf.keras.layers.Conv2D(64, 2, activation='relu'))\n",
        "    model.add(tf.keras.layers.MaxPooling2D((2,2)))\n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "    model.add(tf.keras.layers.Dense(units=1024, activation='relu'))\n",
        "    model.add(tf.keras.layers.Dropout(0.3))\n",
        "    model.add(tf.keras.layers.Dense(units=512, activation='relu'))\n",
        "    model.add(tf.keras.layers.Dropout(0.2))\n",
        "    model.add(tf.keras.layers.Dense(units=10, activation='softmax'))     \n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(lr=learning_rate),\n",
        "                    loss=\"sparse_categorical_crossentropy\",\n",
        "                    metrics=['accuracy']) \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wm2tbc8OqSDY"
      },
      "source": [
        "model_C = create_mixedNN(learning_rate)\n",
        "model_C.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxBUp0wMs31c"
      },
      "source": [
        "model_C.layers[-3].output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMfitb4zTuiS"
      },
      "source": [
        "def getDNNModel(learning_rate):\n",
        "    inputs = keras.Input(shape=(28, 28, 1))\n",
        "    dnn = layers.Flatten()(inputs)\n",
        "    dnn = layers.Dense(768, activation='relu')(dnn)\n",
        "    dnn = layers.Dropout(0.3)(dnn)\n",
        "    dnn = layers.Dense(units=512, activation='relu')(dnn)\n",
        "    dnn = layers.Dropout(0.2)(dnn)\n",
        "    dnn = layers.Dense(units=256, activation='relu')(dnn)\n",
        "    dnn = layers.Dropout(0.1)(dnn)\n",
        "    outputs = layers.Dense(10, activation=\"softmax\")(dnn)\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    model.compile(optimizer=keras.optimizers.Adam(lr=learning_rate),\n",
        "                    loss=\"sparse_categorical_crossentropy\",\n",
        "                    metrics=['accuracy']) \n",
        "    return model    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "collapsed": true,
        "id": "Nn2eBWKXTuaX"
      },
      "source": [
        "# Train and evalate CNN on ALL the datasets, \n",
        "# Train on 10000 + 60000 + 42000 to predict on 28000\n",
        "learning_rate = 0.001\n",
        "epochs = 50\n",
        "batch_size = 300\n",
        "\n",
        "if submit:\n",
        "    validation_split = None\n",
        "    verbose = 0\n",
        "else:\n",
        "    validation_split = 0.1 # off bottom\n",
        "    verbose = 1\n",
        "start_time = time.time()\n",
        "\n",
        "model_A = create_CNN(learning_rate)\n",
        "model_B = getDNNModel(learning_rate)\n",
        "model_C = create_mixedNN(learning_rate)\n",
        "\n",
        "#x_test_norm:\n",
        "epochs_A, hist_A = train_model(model_A, x_test_norm, y_test, epochs, batch_size, validation_split, verbose=verbose)\n",
        "if not submit:\n",
        "    print(\"\\nTrained model A in about {} seconds\\n\".format((round(time.time()-start_time,4))))\n",
        "batch_time = time.time()\n",
        "\n",
        "#x_train_norm:\n",
        "epochs_B, hist_B = train_model(model_B, x_test_norm, y_test, epochs, batch_size, validation_split, verbose=verbose)\n",
        "if not submit:\n",
        "    print(\"\\nTrained model B in about {} seconds\\n\".format((round(time.time()-start_time,4))))\n",
        "batch_time = time.time()\n",
        "\n",
        "#k_train_norm:\n",
        "epochs_C, hist_C = train_model(model_C, x_test_norm, y_test, epochs, batch_size, validation_split, verbose=verbose)\n",
        "if not submit:\n",
        "    print(\"\\nTrained model C in about {} seconds\\n\".format((round(time.time()-start_time,4))))\n",
        "\n",
        "if not submit:\n",
        "    print (\"\\nTotal time: {} seconds is about {} minutes.\".format(round(time.time()-start_time,4),\n",
        "                                                              (time.time()-start_time)//60))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kk96OgxoP5jp"
      },
      "source": [
        "model_A.save('model_A1')\n",
        "model_B.save('model_B1')\n",
        "model_C.save('model_C1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XaLUPOicU9x"
      },
      "source": [
        "if not submit:\n",
        "    print (\"Evaluate against x_test_norm:\")\n",
        "    print (\"A:\",model_A.evaluate(x_test_norm, y_test, batch_size=100))\n",
        "    print (\"B:\",model_B.evaluate(x_test_norm, y_test, batch_size=100))\n",
        "    print (\"C:\",model_C.evaluate(x_test_norm, y_test, batch_size=100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQoJ-mA6SpPF"
      },
      "source": [
        "def getUberModel(learning_rate, modelA, modelB, modelC):\n",
        "    \n",
        "    inputs = keras.Input(shape=(28, 28, 1))\n",
        "    xA = modelA(inputs)\n",
        "    xB = modelB(inputs)\n",
        "    xC = modelC(inputs)\n",
        "    x = layers.concatenate([xA, xB, xC])\n",
        "    x = layers.Dense(300, activation='relu')(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    outputs = layers.Dense(10, activation=\"softmax\")(x)\n",
        "    ensemble_model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    ensemble_model.compile(optimizer=keras.optimizers.Adam(lr=learning_rate),\n",
        "                            loss=\"sparse_categorical_crossentropy\",\n",
        "                            metrics=['accuracy']) \n",
        "    return ensemble_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-GQdo-oel-f"
      },
      "source": [
        "ubermodel = getUberModel(learning_rate, model_A, model_B, model_C)\n",
        "ubermodel.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFHDTK1C6VSs"
      },
      "source": [
        "keras.utils.plot_model(ubermodel, \"ubermodel.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKTDlv-tP5jp"
      },
      "source": [
        "uberdataA = getProbabilities(model_A, x_test_norm)\n",
        "uberdataB = getProbabilities(model_B, x_test_norm)\n",
        "uberdataC = getProbabilities(model_C, x_test_norm)\n",
        "uberdata = np.zeros((10000,30))\n",
        "uberdata[:,:10] = uberdataA\n",
        "uberdata[:,10:20] = uberdataB\n",
        "uberdata[:,20:30] = uberdataC\n",
        "\n",
        "uberdata = uberdata.reshape(-1,30,1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RuhvEOaMP5jp"
      },
      "source": [
        "#Ubermodel train:\n",
        "\n",
        "epochs_U, hist_U = train_model(ubermodel, x_test_norm, y_test, epochs, batch_size, validation_split, verbose=verbose)\n",
        "\n",
        "if not submit:\n",
        "    print(\"\\nTrained model C in about {} seconds\\n\".format((round(time.time()-start_time,4))))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5bHGejdaxby"
      },
      "source": [
        "def getPredictions(model=None, dataset=None): \n",
        "    ''' Takes a trained model and data\n",
        "        Returns a List of guesses for that model on that data\n",
        "    ''' \n",
        "    predictions = []\n",
        "    predicts = model.predict(dataset)\n",
        "    for j in range(len(dataset)):\n",
        "        probs = predicts[j] # one row of 10 probabilities \n",
        "        max_id = np.argmax(probs)   # index of top probability in row\n",
        "        predictions += [max_id]\n",
        "    return predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzxZVtm0P5jp"
      },
      "source": [
        "def getProbabilities(model=None, dataset=None):\n",
        "    ''' Takes a trained model and a set of data\n",
        "        Returns a List of guesses by that model for that data\n",
        "    ''' \n",
        "    return model.predict(dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aGtML-Okikx"
      },
      "source": [
        "kaggle = pd.read_csv('../input/digit-recognizer/test.csv')\n",
        "kaggle_norm = np.asarray(kaggle/255.)\n",
        "kaggle_norm = kaggle_norm.reshape(28000, 28, 28, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QBcAX5baRij"
      },
      "source": [
        "kag_guesses = pd.DataFrame(columns=['A', 'B', 'C', 'AB','AC','CB'])\n",
        "\n",
        "kag_guesses['A'] = getPredictions(model=model3D_A, dataset=kaggle_norm)\n",
        "kag_guesses['B'] = getPredictions(model=model3D_B, dataset=kaggle_norm)\n",
        "kag_guesses['C'] = getPredictions(model=model3D_C, dataset=kaggle_norm)\n",
        "\n",
        "for i in range(len(kag_guesses['A'])):\n",
        "    kag_guesses['AB'] = kag_guesses['A'] - kag_guesses['B']\n",
        "    kag_guesses['AC'] = kag_guesses['A'] - kag_guesses['C']\n",
        "    kag_guesses['CB'] = kag_guesses['C'] - kag_guesses['B']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9J2IS3MhP5jq"
      },
      "source": [
        "kag_probs = pd.DataFrame(columns=list(range(30)))\n",
        "\n",
        "getProbabilities()\n",
        "uberguesses = getPredictions(model=ubermodel, )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UUac0XGP5jq"
      },
      "source": [
        "if not submit:\n",
        "#All agree on them all!?\n",
        "    kag_guesses['CB'].sum() + kag_guesses['AC'].sum() + kag_guesses['AB'].sum()\n",
        "    kag_guesses.to_csv('guessesABC.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8mmj0gsxMD0"
      },
      "source": [
        "if submit:\n",
        "    kaggles = pd.DataFrame(columns=['ImageId','Label']) \n",
        "    kaggles['Label'] = kag_guesses['C']\n",
        "    kaggles['ImageId'] = [i+1 for i in kaggles.index.values] \n",
        "    kaggles.to_csv('submission.csv', columns=[\"ImageId\",\"Label\"], index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9H7W-OKHUnsp"
      },
      "source": [
        "##Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSy9WCq7P5jq"
      },
      "source": [
        "k_img = np.asarray(kaggle.iloc[20044 - 1])\n",
        "k_img = k_img.reshape(28,28)\n",
        "plt.imshow(k_img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6QCoQqRvRfb"
      },
      "source": [
        "if not submit:\n",
        "# Plot a graph of the 'accuracy' metric vs. epochs:\n",
        "    plt.plot(epochs_A,hist_A[\"accuracy\"])\n",
        "    plt.plot(epochs_A,hist_A[\"val_accuracy\"])\n",
        "    plt.show()\n",
        "\n",
        "    plt.plot(epochs_B,hist_B[\"accuracy\"])\n",
        "    plt.plot(epochs_B,hist_B[\"val_accuracy\"])\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}