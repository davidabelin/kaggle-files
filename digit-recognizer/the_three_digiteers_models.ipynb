{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "the_three_digiteers_models.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9n9_cTveKmse",
        "cellView": "both",
        "trusted": true
      },
      "source": [
        "import random as rd\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "#submit = True\n",
        "submit = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZlvdpyYKx7V",
        "cellView": "both",
        "trusted": true
      },
      "source": [
        "# load the public MNIST dataset.\n",
        "(x_train, y_train),(x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "x_train = x_train.reshape(60000,784)\n",
        "x_test = x_test.reshape(10000,784)\n",
        "\n",
        "x_train_norm = x_train/255.\n",
        "x_test_norm = x_test/255."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ubww4azcuuHr"
      },
      "source": [
        "k_train = pd.read_csv('/content/train.csv')\n",
        "k_labels = np.asarray(k_train['label'])\n",
        "k_train = k_train.drop(columns=['label'])\n",
        "k_train_norm = np.asarray(k_train/255.)\n",
        "\n",
        "val_set_labels = k_labels[-2000:]\n",
        "val_set_norm = k_train_norm[-2000:]\n",
        "val_set_norm = val_set_norm.reshape(2000, 28, 28, 1)\n",
        "\n",
        "k_train_norm = k_train_norm[:-2000]\n",
        "k_labels = k_labels[:-2000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cO828kSsxzC"
      },
      "source": [
        "x_data = [row for row in x_test_norm]\n",
        "x_data += [row for row in x_train_norm]\n",
        "x_data += [row for row in k_train_norm]\n",
        "x_data = np.asarray(x_data)\n",
        "x_data = x_data.reshape((110000, 28, 28, 1))\n",
        "\n",
        "y_data = [label for label in y_test]\n",
        "y_data += [label for label in y_train]\n",
        "y_data += [label for label in k_labels]\n",
        "y_data = np.asarray(y_data).flatten()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "aBHU76baSzlJ"
      },
      "source": [
        "# train on the training set with some held back for validation #\n",
        "def train_model(model, train_features, train_label, epochs,\n",
        "                batch_size=None, validation_split=None):\n",
        "\n",
        "    history = model.fit(x=train_features, y=train_label, \n",
        "                        batch_size=batch_size,\n",
        "                        epochs=epochs, shuffle=True, \n",
        "                        validation_split=validation_split,\n",
        "                        verbose = 1)\n",
        "\n",
        "    # Gather the model's metrics after each round of training\n",
        "    epochs = history.epoch\n",
        "    hist = pd.DataFrame(history.history)\n",
        "    return epochs, hist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1C_tbFXcMHu7",
        "trusted": true
      },
      "source": [
        " # Set up a CNN with Keras\n",
        " \n",
        "def create_CNN(learning_rate):\n",
        "    \"\"\"Create and compile a convolutional neural net.\"\"\"  \n",
        "    # Define the kind of model to use.\n",
        "    model = tf.keras.models.Sequential()\n",
        "    model.add(tf.keras.layers.Conv2D(32, 6, activation='relu', input_shape=(28, 28, 1)))\n",
        "    model.add(tf.keras.layers.Conv2D(64, 4, activation='relu'))\n",
        "    model.add(tf.keras.layers.Conv2D(128, 2, activation='relu'))\n",
        "    model.add(tf.keras.layers.MaxPooling2D((2,2)))\n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "    model.add(tf.keras.layers.Dense(units=10, activation='softmax'))     \n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(lr=learning_rate),\n",
        "                    loss=\"sparse_categorical_crossentropy\",\n",
        "                    metrics=['accuracy']) \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "as6m33riWYRN"
      },
      "source": [
        "def create_DNN(learning_rate):\n",
        "    \"\"\"Create and compile a deep neural net.\"\"\"  \n",
        "    # Define the kind of model to use.\n",
        "    model = tf.keras.models.Sequential()\n",
        "    model.add(tf.keras.layers.Flatten(input_shape=(28, 28, 1)))\n",
        "    model.add(tf.keras.layers.Dense(units=784, activation='relu'))  \n",
        "    model.add(tf.keras.layers.Dropout(0.3))\n",
        "    model.add(tf.keras.layers.Dense(units=512, activation='relu'))\n",
        "    model.add(tf.keras.layers.Dropout(0.2))\n",
        "    model.add(tf.keras.layers.Dense(units=256, activation='relu'))\n",
        "    model.add(tf.keras.layers.Dropout(0.1))\n",
        "    model.add(tf.keras.layers.Dense(units=10, activation='softmax'))     \n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(lr=learning_rate),\n",
        "                    loss=\"sparse_categorical_crossentropy\",\n",
        "                    metrics=['accuracy']) \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "ViVafL--X5bG"
      },
      "source": [
        "def create_mixedNN(learning_rate):\n",
        "    \"\"\"Create and compile a deep neural net.\"\"\"  \n",
        "    # Define the kind of model to use.\n",
        "    model = tf.keras.models.Sequential()\n",
        "    model.add(tf.keras.layers.Conv2D(32, 6, activation='relu', input_shape=(28, 28, 1)))\n",
        "    model.add(tf.keras.layers.Conv2D(64, 3, activation='relu'))\n",
        "    model.add(tf.keras.layers.MaxPooling2D((2,2)))\n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "    model.add(tf.keras.layers.Dense(units=512, activation='relu'))\n",
        "    model.add(tf.keras.layers.Dropout(0.2))\n",
        "    model.add(tf.keras.layers.Dense(units=256, activation='relu'))\n",
        "    model.add(tf.keras.layers.Dropout(0.1))\n",
        "    model.add(tf.keras.layers.Dense(units=10, activation='softmax'))     \n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(lr=learning_rate),\n",
        "                    loss=\"sparse_categorical_crossentropy\",\n",
        "                    metrics=['accuracy']) \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "id": "Nn2eBWKXTuaX",
        "trusted": true
      },
      "source": [
        "# Train and evalate CNN on ALL the datasets, \n",
        "# Train on 10000 + 60000 + 42000 to predict on 28000\n",
        "learning_rate = 0.001\n",
        "epochs = 40\n",
        "batch_size = 500\n",
        "validation_split = 0.005 # 0.5% off bottom of each array\n",
        "start_time = time.time()\n",
        "\n",
        "model3D_A = create_CNN(learning_rate)\n",
        "model3D_B = create_DNN(learning_rate)\n",
        "model3D_C = create_CNN(learning_rate)\n",
        "\n",
        "#x_test_norm:\n",
        "epochs_A, hist_A = train_model(model3D_A, x_data, y_data, epochs, batch_size, validation_split)\n",
        "print(\"\\nTrained model A in about {} seconds\\n\".format((round(time.time()-start_time,4))))\n",
        "batch_time = time.time()\n",
        "\n",
        "#x_train_norm:\n",
        "epochs_B, hist_B = train_model(model3D_B, x_data, y_data, epochs, batch_size, validation_split)\n",
        "print(\"\\nTrained model B in about {} seconds\\n\".format((round(time.time()-start_time,4))))\n",
        "batch_time = time.time()\n",
        "\n",
        "#k_train_norm:\n",
        "epochs_C, hist_C = train_model(model3D_C, x_data, y_data, epochs, batch_size, validation_split)\n",
        "print(\"\\nTrained model C in about {} seconds\\n\".format((round(time.time()-start_time,4))))\n",
        "\n",
        "print (\"\\nTotal time: {} seconds is about {} minutes.\".format(round(time.time()-start_time,4),\n",
        "                                                              (time.time()-start_time)//60))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XaLUPOicU9x"
      },
      "source": [
        "model3D_C.evaluate(val_set_norm, val_set_labels,batch_size=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5bHGejdaxby"
      },
      "source": [
        "def getPredictions(model=None, dataset=None): \n",
        "    ''' Takes a trained model and data\n",
        "        Returns a List of guesses for that model on that data\n",
        "    ''' \n",
        "    predictions = []\n",
        "    predicts = model.predict(dataset)\n",
        "    for j in range(len(dataset)):\n",
        "        probs = predicts[j] # one row of 10 probabilities \n",
        "        max_id = np.argmax(probs)   # index of top probability in row\n",
        "        predictions += [max_id]\n",
        "    return predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aGtML-Okikx",
        "trusted": true
      },
      "source": [
        "kaggle = pd.read_csv('/content/test.csv')\n",
        "kaggle_norm = np.asarray(kaggle/255.)\n",
        "kaggle_norm = kaggle_norm.reshape(28000, 28, 28, 1)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QBcAX5baRij"
      },
      "source": [
        "kag_guesses = pd.DataFrame(columns=['A', 'B', 'C', 'AB','AC','CB'])\n",
        "kag_guesses['A'] = getPredictions(model=model3D_A, dataset=kaggle_norm)\n",
        "kag_guesses['B'] = getPredictions(model=model3D_B, dataset=kaggle_norm)\n",
        "kag_guesses['C'] = getPredictions(model=model3D_C, dataset=kaggle_norm)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Twu-mcxRYi5v"
      },
      "source": [
        "#%%time\n",
        "for i in range(len(kag_guesses['A'])):\n",
        "    kag_guesses['AB'] = kag_guesses['A'].iloc[i] != kag_guesses['B'].iloc[i]\n",
        "    kag_guesses['AC'] = kag_guesses['A'].iloc[i] != kag_guesses['C'].iloc[i]\n",
        "    kag_guesses['CB'] = kag_guesses['C'].iloc[i] != kag_guesses['B'].iloc[i]"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8Wa6gf9qXm-",
        "outputId": "196259a7-8929-4603-f78e-dab1d662d209"
      },
      "source": [
        "#All agree on them all!\n",
        "kag_guesses['CB'].astype(float).sum() + kag_guesses['AC'].astype(float).sum() + kag_guesses['AB'].astype(float).sum()"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8mmj0gsxMD0"
      },
      "source": [
        "if submit:\n",
        "    kaggles = pd.DataFrame(columns=['ImageId','Label']) \n",
        "    kaggles['Label'] = kag_guesses['A']\n",
        "    kaggles['ImageId'] = [i+1 for i in kaggles.index.values] \n",
        "    kaggles.to_csv('submission.csv', columns=[\"ImageId\",\"Label\"], index=False)"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9H7W-OKHUnsp"
      },
      "source": [
        "##Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6QCoQqRvRfb",
        "trusted": true
      },
      "source": [
        "if not submit:\n",
        "# Plot a graph of the 'accuracy' metric vs. epochs:\n",
        "    plt.plot(epochs_A,hist_A[\"accuracy\"])\n",
        "    plt.plot(epochs_A,hist_A[\"val_accuracy\"])\n",
        "    plt.show()\n",
        "\n",
        "    plt.plot(epochs_B,hist_B[\"accuracy\"])\n",
        "    plt.plot(epochs_B,hist_B[\"val_accuracy\"])\n",
        "    plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}