{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "the_three_digiteers_data.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9n9_cTveKmse",
        "cellView": "both",
        "trusted": true
      },
      "source": [
        "import random as rd\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "#submit = True\n",
        "submit = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZlvdpyYKx7V",
        "cellView": "both",
        "trusted": true
      },
      "source": [
        "# load the public MNIST dataset.\n",
        "(x_train, y_train),(x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "x_train_norm = x_train/255.\n",
        "x_test_norm = x_test/255."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1aGtML-Okikx",
        "trusted": true
      },
      "source": [
        "#kaggle = pd.read_csv('../input/digit-recognizer/test.csv')\n",
        "#kaggle_norm = np.asarray(kaggle/255.)\n",
        "#kaggle_norm = kaggle_norm.reshape(28000, 28, 28, 1)\n",
        "\n",
        "k_train = pd.read_csv('/content/train.csv')\n",
        "k_labels = np.asarray(k_train['label'])\n",
        "k_train = k_train.drop(columns=['label'])\n",
        "k_train_norm = np.asarray(k_train/255.)\n",
        "\n",
        "val_set_norm = k_train_norm[-2000:]\n",
        "val_set_labels = k_labels[-2000:]\n",
        "k_train_norm = k_train_norm[:-2000]\n",
        "k_labels = k_labels[:-2000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CYqd8LMZ75Z",
        "trusted": true
      },
      "source": [
        "# Add a channels dimension\n",
        "k_train_norm = k_train_norm.reshape(40000, 28, 28, 1)\n",
        "x_train_norm = x_train_norm.reshape(60000, 28, 28, 1)\n",
        "x_test_norm = x_test_norm.reshape(10000, 28, 28, 1)\n",
        "val_set_norm = val_set_norm.reshape(2000, 28, 28, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "aBHU76baSzlJ"
      },
      "source": [
        "# train on the training set with some held back for validation #\n",
        "def train_model(model, train_features, train_label, epochs,\n",
        "                batch_size=None, validation_split=None):\n",
        "\n",
        "    history = model.fit(x=train_features, y=train_label, \n",
        "                        batch_size=batch_size,\n",
        "                        epochs=epochs, shuffle=True, \n",
        "                        validation_split=validation_split,\n",
        "                        verbose = 1)\n",
        "\n",
        "    # Gather the model's metrics after each round of training\n",
        "    epochs = history.epoch\n",
        "    hist = pd.DataFrame(history.history)\n",
        "    return epochs, hist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1C_tbFXcMHu7",
        "trusted": true
      },
      "source": [
        " # Set up a CNN with Keras\n",
        " \n",
        "def create_CNN(learning_rate):\n",
        "    \"\"\"Create and compile a convolutional neural net.\"\"\"  \n",
        "    # Define the kind of model to use.\n",
        "    model = tf.keras.models.Sequential()\n",
        "    model.add(tf.keras.layers.Conv2D(32, 6, activation='relu', input_shape=(28, 28, 1)))\n",
        "    model.add(tf.keras.layers.Conv2D(64, 4, activation='relu'))\n",
        "    model.add(tf.keras.layers.Conv2D(128, 2, activation='relu'))\n",
        "    model.add(tf.keras.layers.MaxPooling2D((2,2)))\n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "    model.add(tf.keras.layers.Dense(units=10, activation='softmax'))     \n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(lr=learning_rate),\n",
        "                    loss=\"sparse_categorical_crossentropy\",\n",
        "                    metrics=['accuracy']) \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "id": "Nn2eBWKXTuaX",
        "trusted": true
      },
      "source": [
        "# Train and evalate CNN on ALL the datasets, \n",
        "# Train on 10000 + 60000 + 42000 to predict on 28000\n",
        "learning_rate = 0.001\n",
        "epochs = 40\n",
        "batch_size = 500\n",
        "validation_split = 0.005 # 0.5% off bottom of each array\n",
        "start_time = time.time()\n",
        "\n",
        "x_trained = create_CNN(learning_rate)\n",
        "x_tested = create_CNN(learning_rate)\n",
        "k_trained = create_CNN(learning_rate)\n",
        "\n",
        "#x_test_norm:\n",
        "epochs_CNN1, hist_CNN1 = train_model(x_tested, x_test_norm, y_test, epochs, batch_size, validation_split)\n",
        "print(\"\\nFinished test set of 10000 in about {} seconds\\n\".format((round(time.time()-start_time,4))))\n",
        "batch_time = time.time()\n",
        "\n",
        "#x_train_norm:\n",
        "epochs_CNN2, hist_CNN2 = train_model(x_trained, x_train_norm, y_train, epochs, batch_size, validation_split)\n",
        "print(\"\\nFinished train set of 60000 in about {} seconds\\n\".format((round(time.time()-start_time,4))))\n",
        "batch_time = time.time()\n",
        "\n",
        "#k_train_norm:\n",
        "epochs_CNN3, hist_CNN3 = train_model(k_trained, k_train_norm, k_labels, epochs, batch_size, validation_split)\n",
        "print(\"\\nFinished kaggle set of 40000 in about {} seconds\\n\".format((round(time.time()-start_time,4))))\n",
        "\n",
        "print (\"\\nTotal time: {} seconds is about {} minutes.\".format(round(time.time()-start_time,4),\n",
        "                                                              (time.time()-start_time)//60))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "bMqZmwMnSzlJ"
      },
      "source": [
        "if submit:\n",
        "    \n",
        "    def getKaggles():  \n",
        "        kaggles = pd.DataFrame(columns=['ImageId','Label'])  \n",
        "        predicts = convoluter.predict(kaggle_norm)\n",
        "        for j in range(len(kaggle_norm)):\n",
        "            probs = predicts[j] # one row of 10 probabilities \n",
        "            max_id = np.argmax(probs)   # index of top probability in row\n",
        "            kaggles.at[j,'ImageId'] = j+1\n",
        "            kaggles.at[j,'Label'] = max_id\n",
        "        return kaggles\n",
        "    \n",
        "    kaggles = getKaggles()\n",
        "    kaggles.to_csv('submission.csv', columns=[\"ImageId\",\"Label\"], index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W5bHGejdaxby"
      },
      "source": [
        "def getPredictions(model=None, dataset=None): \n",
        "    ''' Takes a trained model and data\n",
        "        Returns a List of guesses for that model on that data\n",
        "    ''' \n",
        "    predictions = []\n",
        "    predicts = model.predict(dataset)\n",
        "    for j in range(len(dataset)):\n",
        "        probs = predicts[j] # one row of 10 probabilities \n",
        "        max_id = np.argmax(probs)   # index of top probability in row\n",
        "        predictions += [max_id]\n",
        "    return predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Twu-mcxRYi5v"
      },
      "source": [
        "val_xtrain = getPredictions(model=x_trained, dataset=val_set_norm)\n",
        "val_xtest = getPredictions(model=x_tested, dataset=val_set_norm)\n",
        "val_ktrain = getPredictions(model=k_trained, dataset=val_set_norm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4QBcAX5baRij"
      },
      "source": [
        "val_guesses = pd.DataFrame(columns=['label', 'xtrain', 'xtest', 'ktrain'])\n",
        "val_guesses['label'] = val_set_labels\n",
        "val_guesses['xtrain'] = val_xtrain\n",
        "val_guesses['xtest'] = val_xtest\n",
        "val_guesses['ktrain'] = val_ktrain\n",
        "val_guesses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-iQD8DDgEp6"
      },
      "source": [
        "tv0 = val_guesses['xtrain'] != val_guesses['label']\n",
        "tv1 = val_guesses['xtest'] != val_guesses['label']\n",
        "tv2 = val_guesses['ktrain'] != val_guesses['label']\n",
        "tv3 = guesses['xtrain'] == guesses['xtest']\n",
        "tv4 = guesses['xtrain'] == guesses['ktrain']\n",
        "tv5 = guesses['ktrain'] == guesses['xtest']\n",
        "\n",
        "#val_guesses.loc[lambda df: df['xtrain'] != df['label']]\n",
        "# or df['xtest'] != df['label']or df['ktrain'] != df['label']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFJ5I-neiwt0"
      },
      "source": [
        "guesses = val_guesses.loc[tv0==True]\n",
        "guesses = guesses.append(val_guesses.loc[tv1==True])\n",
        "guesses = guesses.append(val_guesses.loc[tv2==True])\n",
        "guesses = guesses.drop_duplicates()\n",
        "guesses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGDfP-qkotOA"
      },
      "source": [
        "val_guesses.loc[tv0==True]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PaRJsw3znupY"
      },
      "source": [
        "guesses.loc[tv3==True]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdgP8ojdoCSP"
      },
      "source": [
        "guesses.loc[tv4==True]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0RaPV73oB_3"
      },
      "source": [
        "guesses.loc[tv5==True]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9H7W-OKHUnsp"
      },
      "source": [
        "##Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6QCoQqRvRfb",
        "trusted": true
      },
      "source": [
        "if not submit:\n",
        "# Plot a graph of the 'accuracy' metric vs. epochs:\n",
        "    plt.plot(epochs_CNN3,hist_CNN3[\"accuracy\"])\n",
        "    plt.plot(epochs_CNN3,hist_CNN3[\"val_accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}