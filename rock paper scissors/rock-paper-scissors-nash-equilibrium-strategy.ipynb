{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Rock Paper Scissors - Nash Equilibrium Strategy\n\nExample of using Nash Equilibrium principle in Rock-Paper-Scissors game"},{"metadata":{},"cell_type":"markdown","source":"![](https://storage.googleapis.com/kaggle-competitions/kaggle/22838/logos/header.png?t=2020-11-02-21-55-44)"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"top\"></a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:black; background:#FBE338; border:0' role=\"tab\" aria-controls=\"home\"><center>Quick Navigation</center></h3>\n\n* [1. Nash Equilibrium Overview](#1)\n* [2. Agent Code](#2)\n* [3. Battle Examples](#3)"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"1\"></a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Nash Equilibrium Overview<center><h2>"},{"metadata":{},"cell_type":"markdown","source":"In game theory, the Nash equilibrium, named after the mathematician John Forbes Nash Jr., is a proposed solution of a non-cooperative game involving two or more players in which each player is assumed to know the equilibrium strategies of the other players, and no player has anything to gain by changing only their own strategy. [Wikipedia](https://en.wikipedia.org/wiki/Nash_equilibrium#cite_note-Osborne-1)"},{"metadata":{},"cell_type":"markdown","source":"Consider Rock-Paper-Scissors awards matrix (our reward/action is blue, the reward/action of the opponent is red):"},{"metadata":{},"cell_type":"markdown","source":"![](https://i.imgur.com/aEL9IKd.png)"},{"metadata":{},"cell_type":"markdown","source":"If we played each action with equal probability 1/3 then the opponent must do the same.   \nOtherwise if the opponent will play all the time Rock, then:\n- he ties a third of the time against Rock, \n- he loses a third of the time against Paper,\n- and he wins a third of the time against Scissors.\n\nThen he will get reward 1/3 \\* 0 + 1/3 \\* (-1) + 1/3 * 1 = 0.    \n**But in this case, we can change our strategy to Paper and win all the time.**"},{"metadata":{},"cell_type":"markdown","source":"![](https://i.imgur.com/5FYS8L4.png)"},{"metadata":{},"cell_type":"markdown","source":"If the opponent will play all the time Paper, then:\n- he wins a third of the time against Rock,\n- he ties a third of the time against Paper,\n- and he loses a third of the time against Scissors.\n\nThen he will get reward 1/3 \\* 1 + 1/3 \\* 0 + 1/3 * (-1) = 0.    \n**But in this case, we can change our strategy to Scissors and win all the time.**"},{"metadata":{},"cell_type":"markdown","source":"![](https://i.imgur.com/doHd5dP.png)"},{"metadata":{},"cell_type":"markdown","source":"If the opponent will play all the time Scissors, then:\n- he loses a third of the time against Rock,\n- he wins a third of the time against Paper,\n- and he ties a third of the time against Scissors.\n\nThen he will get reward 1/3 \\* (-1) + 1/3 \\* 1 + 1/3 * (0) = 0.    \n**But in this case, we can change our strategy to Rock and win all the time.**"},{"metadata":{},"cell_type":"markdown","source":"![](https://i.imgur.com/yjy0yCx.png)"},{"metadata":{},"cell_type":"markdown","source":"The remaining option in order to be in equilibrium is that both players need to play a random strategy, then there is no point in changing their strategy - which is the Nash equilibrium"},{"metadata":{},"cell_type":"markdown","source":"Slides and more information: [Game Theory 101: Rock, Paper, Scissors](https://www.youtube.com/watch?v=-1GDMXoMdaY&ab_channel=CrashCourse)"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"2\"></a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent Code<center><h2>"},{"metadata":{},"cell_type":"markdown","source":"To create the agent for this competition, we must put its code in \\*.py file.   \nTo do this, we can use the [magic commands](https://ipython.readthedocs.io/en/stable/interactive/magics.html) of Jupyter Notebooks    \nOne of these commands is [writefile](https://ipython.readthedocs.io/en/stable/interactive/magics.html#cellmagic-writefile) which writes the contents of the cell to a file."},{"metadata":{},"cell_type":"markdown","source":"Let's create an agent that will generate a random number from 0 to 3 each time (Nash Equilibrium Strategy)   \n**You must also put all the necessary imports to the \\*.py file, in our example, this is a RANDOM module**"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"%%writefile submission.py\n\nimport random\n\ndef nash_equilibrium_agent(observation, configuration):\n    return random.randint(0, 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"3\"></a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Battle Examples<center><h2>"},{"metadata":{},"cell_type":"markdown","source":"We need to import the library for creating environments and simulating agent battles"},{"metadata":{"trusted":true},"cell_type":"code","source":"from kaggle_environments import make","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"env = make(\"rps\", configuration={\"episodeSteps\": 1000})","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's create a second agent that will copy our previous action."},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile copy_opponent_agent.py\n\ndef copy_opponent_agent(observation, configuration):\n    if observation.step > 0:\n        return observation.lastOpponentAction\n    else:\n        return 0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's start simulating the battle nash_equilibrium_agent vs copy_opponent_agent"},{"metadata":{"trusted":true},"cell_type":"code","source":"# nash_equilibrium_agent vs copy_opponent_agent\nenv.run([\"submission.py\", \"copy_opponent_agent.py\"])\n\nenv.render(mode=\"ipython\", width=800, height=800)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile reactionary.py\n\nimport random\nfrom kaggle_environments.envs.rps.utils import get_score\n\nlast_react_action = None\n\n\ndef reactionary(observation, configuration):\n    global last_react_action\n    if observation.step == 0:\n        last_react_action = random.randrange(0, configuration.signs)\n    elif get_score(last_react_action, observation.lastOpponentAction) <= 1:\n        last_react_action = (observation.lastOpponentAction + 1) % configuration.signs\n\n    return last_react_action","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# nash_equilibrium_agent vs nash_equilibrium_agent\nenv.run([\"submission.py\", \"submission.py\"])\n\nenv.render(mode=\"ipython\", width=800, height=800)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# nash_equilibrium_agent vs nash_equilibrium_agent\nenv.run([\"submission.py\", \"reactionary.py\"])\n\nenv.render(mode=\"ipython\", width=800, height=800)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}