{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "trained_agent_depth4.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-Zbr-B3WPbl"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "So far, our agents have relied on detailed information about how to play the game.  The heuristic really provides a lot of guidance about how to select moves!\n",
        "\n",
        "In this tutorial, you'll learn how to use **reinforcement learning** to build an intelligent agent without the use of a heuristic.  Instead, we will gradually refine the agent's strategy over time, simply by playing the game and trying to maximize the winning rate.\n",
        "\n",
        "In this notebook, we won't be able to explore this complex field in detail, but you'll learn about the big picture and explore code that you can use to train your own agent.\n",
        "\n",
        "# Neural Networks\n",
        "\n",
        "It's difficult to come up with a perfect heuristic.  Improving the heuristic generally entails playing the game many times, to determine specific cases where the agent could have made better choices.  And, it can prove challenging to interpret what exactly is going wrong, and ultimately to fix old mistakes without accidentally introducing new ones.\n",
        "\n",
        "Wouldn't it be much easier if we had a more systematic way of improving the agent with gameplay experience?  \n",
        "\n",
        "In this tutorial, towards this goal, we'll replace the heuristic with a neural network.\n",
        "\n",
        "The network accepts the current board as input.  And, it outputs a probability for each possible move.\n",
        "\n",
        "<center>\n",
        "<img src=\"https://i.imgur.com/KgAliYQ.png\" width=90%><br/>\n",
        "</center>\n",
        "\n",
        "Then, the agent selects a move by sampling from these probabilities.  For instance, for the game board in the image above, the agent selects column 4 with 50% probability.\n",
        "\n",
        "This way, to encode a good gameplay strategy, we need only amend the weights of the network so that _for every possible game board_, it assigns higher probabilities to better moves.\n",
        "\n",
        "At least in theory, that's our goal.  In practice, we won't actually check if that's the case -- since remember that Connect Four has over 4 trillion possible game boards!\n",
        "\n",
        "# Setup\n",
        "\n",
        "How can we approach the task of amending the weights of the network, in practice?  Here's the approach we'll take in this lesson:\n",
        "- After each move, we give the agent a **reward** that tells it how well it did:\n",
        "  - **_If_** the agent wins the game in that move, we give it a reward of `+1`.\n",
        "  - **_Else if_** the agent plays an invalid move (which ends the game), we give it a reward of `-10`.\n",
        "  - **_Else if_** the opponent wins the game in its next move (i.e., the agent failed to prevent its opponent from winning), we give the agent a reward of `-1`.\n",
        "  - **_Else_**, the agent gets a reward of `1/42`.\n",
        "  \n",
        "  \n",
        "- At the end of each game, the agent adds up its reward.  We refer to the sum of rewards as the agent's **cumulative reward**.  \n",
        "  - For instance, if the game lasted 8 moves (each player played four times), and the agent ultimately won, then its cumulative reward is `3*(1/42) + 1`.\n",
        "  - If the game lasted 11 moves (and the opponent went first, so the agent played five times), and the opponent won in its final move, then the agent's cumulative reward is `4*(1/42) - 1`.\n",
        "  - If the game ends in a draw, then the agent played exactly 21 moves, and it gets a cumulative reward of `21*(1/42)`.\n",
        "  - If the game lasted 7 moves and ended with the agent selecting an invalid move, the agent gets a cumulative reward of `3*(1/42) - 10`.\n",
        "  \n",
        "Our goal is to find the weights of the neural network that (on average) maximize the agent's cumulative reward.  \n",
        "\n",
        "This idea of using reward to track the performance of an agent is a core idea in the field of reinforcement learning.  Once we define the problem in this way, we can use any of a variety of reinforcement learning algorithms to produce an agent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDyozjYYWPbm"
      },
      "source": [
        "# Reinforcement Learning\n",
        "\n",
        "There are many different reinforcement learning algorithms, such as DQN, A2C, and PPO, among others.  All of these algorithms use a similar process to produce an agent:\n",
        "\n",
        "- Initially, the weights are set to random values.\n",
        "\n",
        "\n",
        "- As the agent plays the game, the algorithm continually tries out new values for the weights, to see how the cumulative reward is affected, on average.  Over time, after playing many games, we get a good idea of how the weights affect cumulative reward, and the algorithm settles towards weights that performed better.  \n",
        "    - _Of course, we have glossed over the details here, and there's a lot of complexity involved in this process.  For now, we focus on the big picture!_\n",
        "    \n",
        "    \n",
        "- This way, we'll end up with an agent that tries to win the game (so it gets the final reward of `+1`, and avoids the `-1` and `-10`) and tries to make the game last as long as possible (so that it collects the `1/42` bonus as many times as it can).\n",
        "    - _You might argue that it doesn't really make sense to want the game to last as long as possible -- this might result in a very inefficient agent that doesn't play obvious winning moves early in gameplay.  And, your intuition would be correct -- this will make the agent take longer to play a winning move!  The reason we include the `1/42` bonus is to help the algorithms we'll use to converge better.  Further discussion is outside of the scope of this course, but you can learn more by reading about the \"temporal credit assignment problem\" and \"reward shaping\"._\n",
        "    \n",
        "In the next section, we'll use the [**Proximal Policy Optimization (PPO)**](https://openai.com/blog/openai-baselines-ppo/) algorithm to create an agent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Qamt6n2WPbn"
      },
      "source": [
        "# Code\n",
        "\n",
        "There are a lot of great implementations of reinforcement learning algorithms online.  In this course, we'll use [Stable Baselines](https://github.com/hill-a/stable-baselines).\n",
        "\n",
        "Currently, Stable Baselines is not yet compatible with TensorFlow 2.0.  So, we begin by downgrading to TensorFlow 1.0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "_kg_hide-output": true,
        "trusted": true,
        "id": "8Ap0aaOlWPbn"
      },
      "source": [
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "pfI59ppiWPbs",
        "collapsed": true
      },
      "source": [
        "# Check version of tensorflow\n",
        "!pip install 'tensorflow==1.15.0'\n",
        "import tensorflow as tf\n",
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jskt3UQVWPbw"
      },
      "source": [
        "To learn more about how to define environments, check out the documentation [here](https://stable-baselines.readthedocs.io/en/master/guide/custom_env.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AQTAeyKW8Eg",
        "trusted": true,
        "collapsed": true
      },
      "source": [
        "!pip install kaggle_environments"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "xSpsFGkUWPbz",
        "outputId": "aab43a28-416b-4eb3-ada7-f5c6ecf6d373",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from kaggle_environments import make, evaluate, agent\n",
        "from gym import spaces\n",
        "\n",
        "class ConnectFourGym:\n",
        "    def __init__(self, agent2=\"random\"):\n",
        "        ks_env = make(\"connectx\", debug=True)\n",
        "        self.env = ks_env.train([None, agent2])\n",
        "        self.rows = ks_env.configuration.rows\n",
        "        self.columns = ks_env.configuration.columns\n",
        "        # Learn about spaces here: http://gym.openai.com/docs/#spaces\n",
        "        self.action_space = spaces.Discrete(self.columns)\n",
        "        self.observation_space = spaces.Box(low=0, high=2, \n",
        "                                            shape=(self.rows,self.columns,1), dtype=np.int)\n",
        "        # Tuple corresponding to the min and max possible rewards\n",
        "        self.reward_range = (-10, 1)\n",
        "        # StableBaselines throws error if these are not defined\n",
        "        self.spec = None\n",
        "        self.metadata = None\n",
        "    def reset(self):\n",
        "        self.obs = self.env.reset()\n",
        "        return np.array(self.obs['board']).reshape(self.rows,self.columns,1)\n",
        "    def change_reward(self, old_reward, done):\n",
        "        if old_reward == 1: # The agent won the game\n",
        "            return 1\n",
        "        elif done: # The opponent won the game\n",
        "            return -1\n",
        "        else: # Reward 1/42\n",
        "            return 1/(self.rows*self.columns)\n",
        "    def step(self, action):\n",
        "        # Check if agent's move is valid\n",
        "        is_valid = (self.obs['board'][int(action)] == 0)\n",
        "        if is_valid: # Play the move\n",
        "            self.obs, old_reward, done, _ = self.env.step(int(action))\n",
        "            reward = self.change_reward(old_reward, done)\n",
        "        else: # End the game and penalize agent\n",
        "            reward, done, _ = -10, True, {}\n",
        "        return np.array(self.obs['board']).reshape(self.rows,self.columns,1), reward, done, _"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading environment football failed: No module named 'gfootball'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ae4Um5JRWPb4"
      },
      "source": [
        "In this notebook, we'll train an agent to beat the **HEURISTIC** agent.  We specify this opponent in the `agent2` argument below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "yw5OkS8Nltpk",
        "cellView": "form"
      },
      "source": [
        "#@title Heuristic Agent\n",
        "def heuristic(obs, config):\n",
        "\n",
        "    ################################\n",
        "    # Imports and helper functions #\n",
        "    ################################\n",
        "    \n",
        "    import numpy as np\n",
        "    import random\n",
        "    import time\n",
        "    \n",
        "    # How deep to make the game tree: higher values take longer to run!\n",
        "    # lookahead depth:\n",
        "    N_STEPS = 2#@param {type:\"integer\"}\n",
        "\n",
        "    # Gets board at next step if agent drops piece in selected column\n",
        "    def drop_piece(grid, col, mark, config):\n",
        "        next_grid = grid.copy()\n",
        "        for row in range(config.rows-1, -1, -1):\n",
        "            if next_grid[row][col] == 0:\n",
        "                break\n",
        "        next_grid[row][col] = mark\n",
        "        return next_grid\n",
        "\n",
        "    # Helper function for get_heuristic: checks if window satisfies heuristic conditions\n",
        "    def check_window(window, num_discs, piece, config):\n",
        "        return (window.count(piece) == num_discs and window.count(0) == config.inarow-num_discs)\n",
        "\n",
        "    # Helper function for get_heuristic: counts number of windows satisfying specified heuristic conditions\n",
        "    def count_windows(grid, num_discs, piece, config):\n",
        "        num_windows = 0\n",
        "        # horizontal\n",
        "        for row in range(config.rows):\n",
        "            for col in range(config.columns-(config.inarow-1)):\n",
        "                window = list(grid[row, col:col+config.inarow])\n",
        "                if check_window(window, num_discs, piece, config):\n",
        "                    num_windows += 1\n",
        "        # vertical\n",
        "        for row in range(config.rows-(config.inarow-1)):\n",
        "            for col in range(config.columns):\n",
        "                window = list(grid[row:row+config.inarow, col])\n",
        "                if check_window(window, num_discs, piece, config):\n",
        "                    num_windows += 1\n",
        "        # positive diagonal\n",
        "        for row in range(config.rows-(config.inarow-1)):\n",
        "            for col in range(config.columns-(config.inarow-1)):\n",
        "                window = list(grid[range(row, row+config.inarow), range(col, col+config.inarow)])\n",
        "                if check_window(window, num_discs, piece, config):\n",
        "                    num_windows += 1\n",
        "        # negative diagonal\n",
        "        for row in range(config.inarow-1, config.rows):\n",
        "            for col in range(config.columns-(config.inarow-1)):\n",
        "                window = list(grid[range(row, row-config.inarow, -1), range(col, col+config.inarow)])\n",
        "                if check_window(window, num_discs, piece, config):\n",
        "                    num_windows += 1\n",
        "        return num_windows\n",
        "    \n",
        "    # Helper function for minimax: calculates value of heuristic for grid\n",
        "    A = 2\n",
        "    B = 20\n",
        "    C = -1\n",
        "    D = -10\n",
        "    def get_heuristic(grid, mark, config):\n",
        "        num_threes = count_windows(grid, 3, mark, config) #A\n",
        "        num_fours = count_windows(grid, 4, mark, config)  #B\n",
        "        num_threes_opp = count_windows(grid, 3, mark%2+1, config) #C\n",
        "        num_fours_opp = count_windows(grid, 4, mark%2+1, config)  #D\n",
        "        score = A*num_threes + C*num_threes_opp + D*num_fours_opp + B*num_fours\n",
        "        return score\n",
        "\n",
        "    # Helper function for minimax: checks if agent or opponent has four in a row in the window\n",
        "    def is_terminal_window(window, config):\n",
        "        return window.count(1) == config.inarow or window.count(2) == config.inarow\n",
        "\n",
        "    # Helper function for minimax: checks if game has ended\n",
        "    def is_terminal_node(grid, config):\n",
        "        # Check for draw \n",
        "        if list(grid[0, :]).count(0) == 0:\n",
        "            return True\n",
        "        # Check for win: horizontal, vertical, or diagonal\n",
        "        # horizontal \n",
        "        for row in range(config.rows):\n",
        "            for col in range(config.columns-(config.inarow-1)):\n",
        "                window = list(grid[row, col:col+config.inarow])\n",
        "                if is_terminal_window(window, config):\n",
        "                    return True\n",
        "        # vertical\n",
        "        for row in range(config.rows-(config.inarow-1)):\n",
        "            for col in range(config.columns):\n",
        "                window = list(grid[row:row+config.inarow, col])\n",
        "                if is_terminal_window(window, config):\n",
        "                    return True\n",
        "        # positive diagonal\n",
        "        for row in range(config.rows-(config.inarow-1)):\n",
        "            for col in range(config.columns-(config.inarow-1)):\n",
        "                window = list(grid[range(row, row+config.inarow), range(col, col+config.inarow)])\n",
        "                if is_terminal_window(window, config):\n",
        "                    return True\n",
        "        # negative diagonal\n",
        "        for row in range(config.inarow-1, config.rows):\n",
        "            for col in range(config.columns-(config.inarow-1)):\n",
        "                window = list(grid[range(row, row-config.inarow, -1), range(col, col+config.inarow)])\n",
        "                if is_terminal_window(window, config):\n",
        "                    return True\n",
        "        return False\n",
        "\n",
        "    # Minimax implementation\n",
        "    def minimax(node, depth, maximizingPlayer, mark, config):\n",
        "        is_terminal = is_terminal_node(node, config)\n",
        "        valid_moves = [c for c in range(config.columns) if node[0][c] == 0]\n",
        "        if depth == 0 or is_terminal:\n",
        "            return get_heuristic(node, mark, config)\n",
        "        if maximizingPlayer:\n",
        "            value = -np.Inf\n",
        "            for col in valid_moves:\n",
        "                child = drop_piece(node, col, mark, config)\n",
        "                value = max(value, minimax(child, depth-1, False, mark, config))\n",
        "            return value\n",
        "        else:\n",
        "            value = np.Inf\n",
        "            for col in valid_moves:\n",
        "                child = drop_piece(node, col, mark%2+1, config)\n",
        "                value = min(value, minimax(child, depth-1, True, mark, config))\n",
        "            return value\n",
        "        \n",
        "    # Uses minimax to calculate value of dropping piece in selected column\n",
        "    def score_move(grid, col, mark, config, nsteps):\n",
        "        next_grid = drop_piece(grid, col, mark, config)\n",
        "        score = minimax(next_grid, nsteps-1, False, mark, config)\n",
        "        return score\n",
        "    \n",
        "    #########################\n",
        "    # Agent makes selection #\n",
        "    #########################\n",
        "    \n",
        "    # Get list of valid moves\n",
        "    valid_moves = [c for c in range(config.columns) if obs.board[c] == 0]\n",
        "    \n",
        "    # Convert the board to a 2D grid\n",
        "    grid = np.asarray(obs.board).reshape(config.rows, config.columns)\n",
        "    \n",
        "    # Use the heuristic to assign a score to each possible board in the next step\n",
        "    scores = dict(zip(valid_moves, [score_move(grid, col, obs.mark, config, N_STEPS) for col in valid_moves]))\n",
        "    \n",
        "    # Get a list of columns (moves) that maximize the heuristic\n",
        "    max_cols = [key for key in scores.keys() if scores[key] == max(scores.values())]\n",
        "    \n",
        "    # Select at random from the maximizing columns\n",
        "    return random.choice(max_cols)"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "477f5962-1948-4b04-b00f-36327ae7e9ed",
        "_uuid": "74733e09-3c25-4a93-b184-8d068c606e18",
        "execution": {
          "iopub.execute_input": "2020-11-02T20:00:26.493248Z",
          "iopub.status.busy": "2020-11-02T20:00:26.488014Z",
          "iopub.status.idle": "2020-11-02T20:00:26.614408Z",
          "shell.execute_reply": "2020-11-02T20:00:26.613675Z"
        },
        "papermill": {
          "duration": 0.146016,
          "end_time": "2020-11-02T20:00:26.614567",
          "exception": false,
          "start_time": "2020-11-02T20:00:26.468551",
          "status": "completed"
        },
        "tags": [],
        "cellView": "form",
        "id": "OH5tEfFlBbfj"
      },
      "source": [
        "# @title Experimental Pruning Agent \"pruneX\"\n",
        "def pruneX(obs, config):\n",
        "    #config is dict: {'rows': 6, 'columns': 7, 'inarow': 4}\n",
        "    # obs.board is last move of opponent, obs.mark is current player\n",
        "    # return column that max's next grid's score\n",
        "\n",
        "    ################################\n",
        "    # Imports and helper functions #\n",
        "    ################################\n",
        "\n",
        "    import numpy as np\n",
        "    import random\n",
        "    import time\n",
        "\n",
        "    ########################### Regular pruner ################\n",
        "    # constants (given by game)\n",
        "    ROWS = 6\n",
        "    COLUMNS = 7\n",
        "    CNCTX = 4\n",
        "    ## coefficients (weights on variable future outcomes)\n",
        "    A = 10       #2 threes\n",
        "    B = 1000     #10 fours\n",
        "    C = -1      #-1 opp-threes\n",
        "    D = -100     #-10 opp-fours   \n",
        "\n",
        "        # lookahead depth:\n",
        "    N_STEPS = 4#@ param {type:\"integer\"}\n",
        "\n",
        "    # Gets board at next step if agent drops piece in selected column\n",
        "    def drop_piece(grid, col, mark):\n",
        "        next_grid = grid.copy()\n",
        "        for row in range(ROWS-1, -1, -1):       ###row in range(0,ROWS)??\n",
        "            if next_grid[row][col] == 0:\n",
        "                break\n",
        "        next_grid[row][col] = mark\n",
        "        return next_grid\n",
        "\n",
        "    # Helper function for get_heuristic: checks if window satisfies heuristic conditions\n",
        "    def check_window(window, num_discs, piece):\n",
        "        return (window.count(piece) == num_discs and window.count(0) == CNCTX-num_discs)\n",
        "\n",
        "    # Helper function for get_heuristic: counts number of windows satisfying specified heuristic conditions\n",
        "    def count_windows(grid, num_discs, piece):\n",
        "        num_windows = 0\n",
        "        # horizontal\n",
        "        for row in range(ROWS):\n",
        "            for col in range(COLUMNS-(CNCTX-1)):\n",
        "                window = list(grid[row, col:col+CNCTX])\n",
        "                if check_window(window, num_discs, piece):\n",
        "                    num_windows += 1\n",
        "        # vertical\n",
        "        for row in range(ROWS-(CNCTX-1)):\n",
        "            for col in range(COLUMNS):\n",
        "                window = list(grid[row:row+CNCTX, col])\n",
        "                if check_window(window, num_discs, piece):\n",
        "                    num_windows += 1\n",
        "        # positive diagonal\n",
        "        for row in range(ROWS-(CNCTX-1)):\n",
        "            for col in range(COLUMNS-(CNCTX-1)):\n",
        "                window = list(grid[range(row, row+CNCTX), range(col, col+CNCTX)])\n",
        "                if check_window(window, num_discs, piece):\n",
        "                    num_windows += 1\n",
        "        # negative diagonal\n",
        "        for row in range(CNCTX-1, ROWS):\n",
        "            for col in range(COLUMNS-(CNCTX-1)):\n",
        "                window = list(grid[range(row, row-CNCTX, -1), range(col, col+CNCTX)])\n",
        "                if check_window(window, num_discs, piece):\n",
        "                    num_windows += 1\n",
        "        return num_windows\n",
        "\n",
        "    # Helper function for minimax: calculates value of heuristic for grid\n",
        "    def get_score(grid, mark):\n",
        "        num_threes = count_windows(grid, 3, mark) #A\n",
        "        num_fours = count_windows(grid, 4, mark)  #B\n",
        "        num_threes_opp = count_windows(grid, 3, mark%2+1) #C\n",
        "        num_fours_opp = count_windows(grid, 4, mark%2+1)  #D\n",
        "\n",
        "        score = A*num_threes + B*num_fours + C*num_threes_opp + D*num_fours_opp\n",
        "        is_terminal = (not num_fours == 0) or (not num_fours_opp == 0) or (list(grid[0, :]).count(0) == 0)\n",
        "        return score, is_terminal\n",
        "\n",
        "    # Minimax implementation was here:\n",
        "    def alphabeta(node, depth, alpha, beta, maximizingPlayer, mark):\n",
        "\n",
        "        node_score, is_terminal = get_score(node, mark)\n",
        "        if depth == 0 or is_terminal:\n",
        "            return node_score\n",
        "        \n",
        "        valid_moves = [c for c in range(COLUMNS) if node[0][c] == 0]\n",
        "        \n",
        "        if maximizingPlayer:\n",
        "            value = -np.Inf\n",
        "            for col in valid_moves:\n",
        "                child = drop_piece(node, col, mark)\n",
        "                value = max(value, alphabeta(child, depth-1, alpha, beta, False, mark))\n",
        "                alpha = max(alpha, value)\n",
        "                if alpha >= beta or (value >= (B + 2*C)):\n",
        "                    break\n",
        "            return value\n",
        "        \n",
        "        else: #minimizing player\n",
        "            value = np.Inf\n",
        "            for col in valid_moves:\n",
        "                child = drop_piece(node, col, mark%2+1)\n",
        "                value = min(value, alphabeta(child, depth-1, alpha, beta, True, mark))\n",
        "                beta = min(beta, value)\n",
        "                if alpha >= beta or (value <= (D + 2*A)):\n",
        "                    break\n",
        "            return value\n",
        "\n",
        "    # Uses minimax to calculate value of dropping piece in selected column\n",
        "    def score_move(grid, col, mark, nsteps):\n",
        "        next_grid = drop_piece(grid, col, mark)\n",
        "        score = alphabeta(next_grid, nsteps-1, -np.Inf, np.Inf, False, mark)     \n",
        "        return score\n",
        "\n",
        "    #########################\n",
        "    # Agent makes selection #\n",
        "    #########################\n",
        "\n",
        "    # Get list of valid moves\n",
        "    valid_moves = [c for c in range(COLUMNS) if obs.board[c] == 0]\n",
        "\n",
        "    # Convert the board to a 2D grid\n",
        "    grid = np.asarray(obs.board).reshape(ROWS, COLUMNS)\n",
        "\n",
        "    #Use the heuristic to assign a score to each possible board in the next step\n",
        "    scores = dict(zip(valid_moves, [score_move(grid, col, mark, N_STEPS) for col in valid_moves]))\n",
        "    returning_scores = []\n",
        "    for col in valid_moves:\n",
        "        col_score = score_move(grid, col, mark, N_STEPS)\n",
        "        returning_scores.append(col_score)\n",
        "        if col_score >= A:# or col_score <= D:\n",
        "            break\n",
        "    #scores = dict(zip(valid_moves, returning_scores))   \n",
        "\n",
        "    # Get a list of columns (moves) that maximize the heuristic\n",
        "    max_cols = [key for key in scores.keys() if scores[key] == max(scores.values())]\n",
        "    max_choice = random.choice(max_cols)\n",
        "\n",
        "    return max_choice"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "477f5962-1948-4b04-b00f-36327ae7e9ed",
        "_uuid": "74733e09-3c25-4a93-b184-8d068c606e18",
        "execution": {
          "iopub.execute_input": "2020-11-02T20:00:26.493248Z",
          "iopub.status.busy": "2020-11-02T20:00:26.488014Z",
          "iopub.status.idle": "2020-11-02T20:00:26.614408Z",
          "shell.execute_reply": "2020-11-02T20:00:26.613675Z"
        },
        "papermill": {
          "duration": 0.146016,
          "end_time": "2020-11-02T20:00:26.614567",
          "exception": false,
          "start_time": "2020-11-02T20:00:26.468551",
          "status": "completed"
        },
        "tags": [],
        "cellView": "form",
        "id": "n8vwzgCUn-dQ"
      },
      "source": [
        "#@title Current Pruning Agent \"pruner\" >>> my_agent\n",
        "def pruner(obs, config):\n",
        "    #config is dict: {'rows': 6, 'columns': 7, 'inarow': 4}\n",
        "    # obs.board is last move of opponent, obs.mark is current player\n",
        "    # return column that max's next grid's score\n",
        "\n",
        "    ################################\n",
        "    # Imports and helper functions #\n",
        "    ################################\n",
        "\n",
        "    import numpy as np\n",
        "    import random\n",
        "\n",
        "    # constants\n",
        "    ROWS = config.rows\n",
        "    COLUMNS = config.columns\n",
        "    CNCTX = config.inarow\n",
        "    A = 10       #2 threes\n",
        "    B = 1000     #10 fours\n",
        "    C = -1      #-1 opp-threes\n",
        "    D = -100    #-10opp-fours   \n",
        "\n",
        "    # lookahead depth:\n",
        "    N_STEPS =   4#@param {type: \"number\"}\n",
        "\n",
        "    # Gets board at next step if agent drops piece in selected column\n",
        "    def drop_piece(grid, col, mark):\n",
        "        next_grid = grid.copy()\n",
        "        for row in range(ROWS-1, -1, -1):       ###row in range(0,ROWS)??\n",
        "            if next_grid[row][col] == 0:\n",
        "                break\n",
        "        next_grid[row][col] = mark\n",
        "        return next_grid\n",
        "\n",
        "    # Helper function for get_heuristic: checks if window satisfies heuristic conditions\n",
        "    def check_window(window, num_discs, piece):\n",
        "        return (window.count(piece) == num_discs and window.count(0) == CNCTX-num_discs)\n",
        "\n",
        "    # Helper function for get_heuristic: counts number of windows satisfying specified heuristic conditions\n",
        "    def count_windows(grid, num_discs, piece):\n",
        "        num_windows = 0\n",
        "        # horizontal\n",
        "        for row in range(ROWS):\n",
        "            for col in range(COLUMNS-(CNCTX-1)):\n",
        "                window = list(grid[row, col:col+CNCTX])\n",
        "                if check_window(window, num_discs, piece):\n",
        "                    num_windows += 1\n",
        "        # vertical\n",
        "        for row in range(ROWS-(CNCTX-1)):\n",
        "            for col in range(COLUMNS):\n",
        "                window = list(grid[row:row+CNCTX, col])\n",
        "                if check_window(window, num_discs, piece):\n",
        "                    num_windows += 1\n",
        "        # positive diagonal\n",
        "        for row in range(ROWS-(CNCTX-1)):\n",
        "            for col in range(COLUMNS-(CNCTX-1)):\n",
        "                window = list(grid[range(row, row+CNCTX), range(col, col+CNCTX)])\n",
        "                if check_window(window, num_discs, piece):\n",
        "                    num_windows += 1\n",
        "        # negative diagonal\n",
        "        for row in range(CNCTX-1, ROWS):\n",
        "            for col in range(COLUMNS-(CNCTX-1)):\n",
        "                window = list(grid[range(row, row-CNCTX, -1), range(col, col+CNCTX)])\n",
        "                if check_window(window, num_discs, piece):\n",
        "                    num_windows += 1\n",
        "        return num_windows\n",
        "\n",
        "    # Helper function for minimax: calculates value of heuristic for grid\n",
        "    def get_score(grid, mark):\n",
        "\n",
        "        num_threes = count_windows(grid, 3, mark) #A\n",
        "        num_fours = count_windows(grid, 4, mark)  #B\n",
        "        num_threes_opp = count_windows(grid, 3, mark%2+1) #C\n",
        "        num_fours_opp = count_windows(grid, 4, mark%2+1)  #D\n",
        "\n",
        "        score = A*num_threes + B*num_fours + C*num_threes_opp + D*num_fours_opp\n",
        "        is_terminal = (not num_fours == 0) or (not num_fours_opp == 0) or (list(grid[0, :]).count(0) == 0)\n",
        "        return score, is_terminal\n",
        "\n",
        "    # Alpha Beta Pruning of MiniMax algorithm:\n",
        "    def alphabeta(node, depth, alpha, beta, maximizingPlayer, mark):\n",
        "        node_score, is_terminal = get_score(node, mark)\n",
        "        if depth == 0 or is_terminal:\n",
        "            return node_score\n",
        " \n",
        "        valid_moves = [c for c in range(COLUMNS) if node[0][c] == 0]\n",
        "        if maximizingPlayer:\n",
        "            value = -np.Inf\n",
        "            for col in valid_moves:\n",
        "                child = drop_piece(node, col, mark)\n",
        "                value = max(value, alphabeta(child, depth-1, alpha, beta, False, mark))\n",
        "                alpha = max(alpha, value)\n",
        "                if alpha >= beta or value >= 900:\n",
        "                    break\n",
        "            return value\n",
        "        \n",
        "        else: #minimizing player\n",
        "            value = np.Inf\n",
        "            for col in valid_moves:\n",
        "                child = drop_piece(node, col, mark%2+1)\n",
        "                value = min(value, alphabeta(child, depth-1, alpha, beta, True, mark))\n",
        "                beta = min(beta, value)\n",
        "                if alpha >= beta or value <= -70:\n",
        "                    break\n",
        "            return value\n",
        "\n",
        "    # Uses minimax to calculate value of dropping piece in selected column\n",
        "    def score_move(grid, col, mark, nsteps):\n",
        "        next_grid = drop_piece(grid, col, mark)\n",
        "        score = alphabeta(next_grid, nsteps-1, -np.Inf, np.Inf, False, mark) \n",
        "        return score\n",
        "\n",
        "    #########################\n",
        "    # Agent makes selection #\n",
        "    #########################\n",
        "\n",
        "    # Get list of valid moves\n",
        "    valid_moves = [c for c in range(COLUMNS) if obs.board[c] == 0]\n",
        "\n",
        "    # Convert the board to a 2D grid\n",
        "    ########## ENTER OBS:\n",
        "    grid = np.asarray(obs.board).reshape(ROWS, COLUMNS)\n",
        "\n",
        "    # Use the heuristic to assign a score to each possible board in the next step\n",
        "    scores = dict(zip(valid_moves, [score_move(grid, col, obs.mark, N_STEPS) for col in valid_moves]))\n",
        "\n",
        "    # Get a list of columns (moves) that maximize the heuristic\n",
        "    max_cols = [key for key in scores.keys() if scores[key] == max(scores.values())]\n",
        "\n",
        "    # Select at random from the maximizing columns\n",
        "    return random.choice(max_cols)"
      ],
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "cellView": "form",
        "id": "PuE-QfS_XmQo",
        "outputId": "e166dbbf-0b5f-4810-c1d6-50313aa5faa9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#@title Time and Score Comparisons\n",
        "start_time = time.time()\n",
        "n_rounds =   1#@param {type:\"integer\"}\n",
        "agent1 = pruner #@param\n",
        "agent2 = pruner #@param\n",
        "get_win_percentages(agent1=agent1, agent2=agent2, n_rounds=n_rounds)\n",
        "print (\"Total time taken: {} seconds (per round: {} seconds)\".format(round(time.time() - start_time, 1), \n",
        "                                                                     round((time.time() - start_time)/n_rounds,3)))"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Agent 1 Win Percentage: 0.0\n",
            "Agent 2 Win Percentage: 1.0\n",
            "Number of Invalid Plays by Agent 1: 0\n",
            "Number of Invalid Plays by Agent 2: 0\n",
            "Total time taken: 34.0 seconds (per round: 34.032 seconds)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "xjPIghBkWPb5"
      },
      "source": [
        "##### Create ConnectFour environment -- define training opponent here!\n",
        "env = ConnectFourGym(agent2=pruner)  ### <--- agent goes here"
      ],
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m13llBZKWPb9"
      },
      "source": [
        "Stable Baselines requires us to work with [\"vectorized\" environments](https://stable-baselines.readthedocs.io/en/master/guide/vec_envs.html).  For this, we can use the `DummyVecEnv` class.  \n",
        "\n",
        "The `Monitor` class lets us watch how the agent's performance gradually improves, as it plays more and more games."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "_kg_hide-output": true,
        "trusted": true,
        "id": "zvhHP182WPb-",
        "collapsed": true
      },
      "source": [
        "!apt-get update\n",
        "!apt-get install -y cmake libopenmpi-dev python3-dev zlib1g-dev\n",
        "!pip install \"stable-baselines[mpi]==2.9.0\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "QWiiBXgJWPcB"
      },
      "source": [
        "import os\n",
        "from stable_baselines.bench import Monitor \n",
        "from stable_baselines.common.vec_env import DummyVecEnv\n",
        "\n",
        "# Create directory for logging training information\n",
        "log_dir = \"ppo/\"\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "# Logging progress\n",
        "monitor_env = Monitor(env, log_dir, allow_early_resets=True)\n",
        "\n",
        "# Create a vectorized environment\n",
        "vec_env = DummyVecEnv([lambda: monitor_env])"
      ],
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lznNOujAWPcE"
      },
      "source": [
        "The next step is to specify the architecture of the neural network.  In this case, we use a convolutional neural network.  To learn more about how to specify architectures with Stable Baselines, check out the documentation [here](https://stable-baselines.readthedocs.io/en/master/guide/custom_policy.html).\n",
        "\n",
        "Note that this is the neural network that outputs the probabilities of selecting each column.  Since we use the PPO algorithm (`PPO1` in the code cell below), our network will also output some additional information (called the \"value\" of the input).  This is outside the scope of this course, but you can learn more by reading about \"actor-critic networks\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caPpkBV4w2IM"
      },
      "source": [
        "from stable_baselines import PPO1 \n",
        "from stable_baselines.a2c.utils import conv, linear, conv_to_fc\n",
        "from stable_baselines.common.policies import CnnPolicy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "qwcjGSbqWPcE"
      },
      "source": [
        "# Neural network for predicting action values\n",
        "def modified_cnn(scaled_images, **kwargs):\n",
        "    activ = tf.nn.relu\n",
        "    layer_1 = activ(conv(scaled_images, 'c1', n_filters=32, filter_size=3, stride=1, \n",
        "                         init_scale=np.sqrt(2), **kwargs))\n",
        "    layer_2 = activ(conv(layer_1, 'c2', n_filters=64, filter_size=3, stride=1, \n",
        "                         init_scale=np.sqrt(2), **kwargs))\n",
        "    layer_2 = conv_to_fc(layer_2)\n",
        "    return activ(linear(layer_2, 'fc1', n_hidden=512, init_scale=np.sqrt(2)))  \n",
        "\n",
        "class CustomCnnPolicy(CnnPolicy):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(CustomCnnPolicy, self).__init__(*args, **kwargs, cnn_extractor=modified_cnn)"
      ],
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZDmcHzWw87i"
      },
      "source": [
        "# Initialize agent\n",
        "model = PPO1(CustomCnnPolicy, vec_env, verbose=0)"
      ],
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "DWw5yfYnWPcI",
        "outputId": "d9a59729-f7a8-4db0-ca15-ea3c4b7751a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Train agent\n",
        "start = time.time()\n",
        "train_steps = 10000\n",
        "model.learn(total_timesteps=train_steps)\n",
        "print (train_steps,\"steps took\",time.time()-start,\"seconds.\")"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000 steps took 13564.445094108582 seconds.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmB5-GV5tPX3"
      },
      "source": [
        "Last: :\n",
        "* 10000 steps took 13564.4 seconds.\n",
        "* 1000 steps took 1367.7 seconds."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "c72lT6e2WPcL",
        "outputId": "48dfe9ba-cd23-4b42-9134-379b96a0314d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "# Plot cumulative reward\n",
        "with open(os.path.join(log_dir, \"monitor.csv\"), 'rt') as fh:    \n",
        "    firstline = fh.readline()\n",
        "    assert firstline[0] == '#'\n",
        "    df = pd.read_csv(fh, index_col=None)['r']\n",
        "df.rolling(window=train_steps//10).mean().plot()\n",
        "plt.show()"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAD4CAYAAADhNOGaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO2deXhc5XX/P2c2LZZsWZZtWbaFbWwwNosNYguBsBgwJg1LQgrZm6SkWdos/JqlpClt0iZNmj1pKEkIJCEkpMRxCiRgwGB2MGDjHe+2vMm2vMiypNHc+/7+uPeORqMZzUizauZ8nkePZu59Z+a9c++833vOe95zxBiDoiiKUr74Ct0BRVEUpbCoECiKopQ5KgSKoihljgqBoihKmaNCoCiKUuYECt2B4dDQ0GCmTZtW6G4oiqKMKF599dWDxpjx8dtHpBBMmzaNFStWFLobiqIoIwoR2ZFou7qGFEVRyhwVAkVRlDJHhUBRFKXMUSFQFEUpc1QIFEVRyhwVAkVRlDJHhUBRFKXMUSFQFEXJMxv3dbDjUCcAEcvmd6/sZP+x7oL1R4VAURQlj+xqP8HV31vO2771FABr9hzjCw+u5rtL3yxYn1QIFEVR8kh7Z7jf866wBcCb+zsK0R0gQyEQkZtEZK2I2CLSMki7u0WkTUTWxG2vF5GlIrLJ/T82k/4oiqIUOxG7f1XInogjBIWsFZmpRbAGuBFYnqLdPcDCBNu/CDxhjJkFPOE+VxRFKVnsuPLA4YhdoJ70kZEQGGPWG2M2ptFuOdCeYNd1wL3u43uB6zPpj6IoSrETseKEwBrhQpAFJhpj9rqP9wETC9kZRVGUXFOMFkHKNNQi8jjQmGDX7caYJdnqiDHGiEhSN5mI3ArcCtDc3Jytj1UURckrsXMEb+7v4KWtjrOk7VhPobqUWgiMMQty+Pn7RWSSMWaviEwC2gbpx13AXQAtLS2FnFdRFEUZNj97Zmv08VXf7Zte3X2kiyfW7+eK0/LvGCm0a+hPwAfdxx8EsmZhKIqijAR+8t6z+YcrZgGws/1EQfqQafjoDSLSClwIPCwij7rbm0TkkZh29wMvAKeKSKuIfMTd9Q3gShHZBCxwnyuKopQslm04d1pfpPzC0xv5u7fNAAo3X5BRqUpjzGJgcYLte4BFMc9vSfL6Q8AVmfRBURRlJNFr2QR8fffgIkLI7zwvlBAU2jWkKIpSVvRahoBf+m0L+H34BHpUCBRFUUqfiG0T9A8cekMBX8HWFKgQKIqi5JHeiCEYZxEAhPw+dQ0piqKUIsYYNrcd56mNbYQjNr22TSCBRWAbeOiNvQneIfdkNFmsKIqiDM6q1qNc/+PnALjzfWcTsQxBn/AfN5zRrwZBT8SioSZUkD6qECiKouSQAx19K4Y7eywiljNH8J7z+2dIWHTGJFbuOpLv7gHqGlIURckpsX5/yxjClknoGtI5AkVRlBIlbFnRx5Zt3KihgZPFFUGfho8qiqKUIv0sAtsQsUy/BWUeIb9fLQJFUZRSJF4IwpZNMJAgfDRQONeQThYriqIMg9bDJ3h1x2EumtlAQ01F0nax7p7dR7oIR2yCiSwCd0HZr17Yzsb9HdRVhXjP+c001VVx8HgPD77ayg3zJzNhdGXWj0WFQFEUZRjc9sAqXtrWzqIzGvnv956TsM3rOw/ztYfXR5/ftdxJQV0V8g9oO39qHQD/vGRt1DoIBXz8wxWzePDVVr7+5w10hi0+d+UpWT8WFQJFUZRhcKw7AsD+QQrK3PDfzwMwu7GWDfs6APirs5p47/kDi2tdNnsCT//jpXT1WsxoqGHOV/5Cd68z0Wy5Vc16eq0Br8sGOkegKIoyDMIRy/2f2q//obdM4/zp9QDcct5U6qoTLxw7adwoZjeOJhTw9Zsz8KpbWnZuanKpECiKogwDL0FcOkLg90k046id5nxwKNAXTuoJQESFQFEUpXjwBCCdjKF+n+B3J4gjaSpBRYxF4AlAUVoEInKTiKwVEVtEWgZpd7eItInImrjt3xKRDSLyhogsFpG6TPqjKIqSL6JCkKZF4K0hS3cwj01Lbdnpf9ZwyNQiWAPcCCxP0e4eYGGC7UuB040xZwJvAl/KsD+Koih5wRuU01kNHPD5ohZB2kLg97HnSBfdvVZ0Qrr1SG5qGmdaqnI9OKXWUrRbLiLTEmx/LObpi8C7MumPoihKrnl200He9/OXos8PHu/hPT99kc6wxWcXzMLvE5a/eYDxtX1rC/w+CLmLyNL17oypCvLStnZm//Nfotue23yIV7a3c+60+uwcjEsxhY9+GPhdsp0icitwK0Bz88DQK0VRlHywcb8TBnrlnImcP72eP67czZv7j3PweA9ffWgdzfXVLNt4oN9r5jaN4eyTxjKmKsTlsyek9Tk/es/ZPLmhjb1Hu2gcXUlF0M+WA8dpLMSCMhF5HGhMsOt2Y8ySbHRCRG4HIsB9ydoYY+4C7gJoaWnJzYyJoihKCjyX0Pdvnkd1KMBHL54BwK2/XMHO9hOEAj5mNIzivr89nzFVQfw+oSLgLCD7+o1npP05TXVVvO+Ck7J/AAlIKQTGmAW57ICIfAh4O3CFMUYHeEVRihpPCEJxqaT9PsGyDZZtqAz6mTSmqhDdGxYFdQ2JyELg88DbjDG5mQVRFEXJImHLctcFJBeCQII008VMpuGjN4hIK3Ah8LCIPOpubxKRR2La3Q+8AJwqIq0i8hF314+AWmCpiKwUkTsz6Y+iKEquCUfsAdYAQMAnWMYQsQ1+38gSgkyjhhYDixNs3wMsinl+S5LXz8zk8xVFUfKNlwwuHp9PiFiOReBPEUlZbBRT1JCiKEpR0tkT4YdPbiYcsbn3hR2MGzUwV9DhzjC7j3Sx92gXLVkO78w1KgSKoigpeG3nYe58egsAk8ZUcuWciQPaTHTDOm0DzfXVee1fpqgQKIqipMCLFFryyYs4a2riTDh3vGMuH3zLNCzbMHNCTT67lzEqBIqiKCnotZzI9sGigSqDfk6bNDpfXcoqmn1UURQlBb1W4rUDpUJpHpWiKEoW8VJHx68dKBVK86gURVGySNQ1NMLWB6SLCoGiKEoKoq6hBOsHSoHSPCpFUZQsElGLQFEUpbzxLAKdI1AURSlTvDkCjRpSFEUpUyJRi0BdQ4qiKGVJr61zBIqiKGVNr2UT9EvK+uwjFRUCRVGUFJzoiZSsCIDmGlIURRmUXe0nuPeFHYXuRk7JtELZTSKyVkRsEWkZpN3dItImImuS7L9NRIyINGTSH0VRlGyzfu8xAD59xawC9yR3ZOoaWgPcCCxP0e4eYGGiHSIyFbgK2JlhXxRFUbJO2I0YuvbMSQXuSe7ISAiMMeuNMRvTaLccaE+y+7s4BexNJn1RFEXJBV4tglJdQwAFniwWkeuA3caYVWm0vVVEVojIigMHDuShd4qiKDFCUKJ5hiCNyWIReRxoTLDrdmPMkuF+sIhUA/+E4xZKiTHmLuAugJaWFrUeFEXJC+ESTzgHaQiBMWZBjj77ZGA6sMoNy5oCvCYi5xlj9uXoMxVFUYZET68KQc4wxqwGJnjPRWQ70GKMOVioPimKosRz/ytOHIvOESRBRG4QkVbgQuBhEXnU3d4kIo/EtLsfeAE4VURaReQjmXyuoihKPrBsw9YDnUwaU0mFWgSJMcYsBhYn2L4HWBTz/JY03mtaJn1RFEXJNt5E8QcunFbSK4tLV+IURVEypBwihkCFQFEUJSk9lgWoECiKopQtnkVQUcITxaBCoCiKkhR1DSmKopQou4908anfvMYPnthET8Rx/xw5EeY7j21kx6FODh7vwbZNWSwmA01DrShKGfLk+v089MZeYC9zJo1mwZyJvLytnR88uZkfPLkZgFkTavjIW6cDUBX0F7C3uae0ZU5RFCUBPa7LB6AzHAEg4pajvOb0RgI+YVPb8Wi7M6aMyX8n84gKgaIoZUesEHiPLVcIPnflKXzisplAnzgEfaU9VJb20SmKoiQgHCME4Tgh8Pskuoq4u9eZPyhxHVAhUBSl/PAmgSGxEHh5hbrCjhAESlwJSvvoFEVREhCO2PjcjBGeKPSzCILO0HjCFQK/r3TTS4BGDSmKUgZsPXCcPUe6CfoFyzb8+sUdBP0+eiI22w508kbrEY529QJxFkGvCoGiKMqIZ/mbB/jA3S8P2D6jYRRbD3byuxW7+N2KXdHtVUF/dN3AkpW7AShxHVAhUBSltNly4DgAX7v+dBpHV9ITsampDDCjYRSrWo+w9UAnghMhdM5JY6mrDnHmlDpOnViLbQwXzxpf0plHQYVAUZQSx5sMvn7+ZGoq+g95U+urE75m5oQaHv3sJTnvW7GQaWGam0RkrYjYItIySLu7RaRNRNYk2Pf3IrLBfZ9vZtIfRVGUeKL5gko8cVwmZPrNrAFuBJanaHcPsDB+o4hcBlwHnGWMmQv8V4b9URRF6YcXFRT0l7Z7JxMyrVC2HkjpPzPGLBeRaQl2fRz4hjGmx23Xlkl/FEVR4glbNqGAr+T9/JlQaFvpFOBiEXlJRJ4WkXML3B9FUUqMcMQu+XoCmZLSIhCRx4HGBLtuN8YsycLn1wMXAOcCD4jIDGOMSdCPW4FbAZqbmzP8WEVRSh1jDKt3H+UXz20vdFeKnpRCYIxZkMPPbwX+4A78L4uIDTQABxL04y7gLoCWlpYBQqEoiuJx9EQvZ/3bY9HnV86ZWMDeFD+FDh/9I3AZsExETgFCwMHCdklRlJHOnqNdAMxvruOXHz6P2spggXtU3GQaPnqDiLQCFwIPi8ij7vYmEXkkpt39wAvAqSLSKiIfcXfdDcxww0p/C3wwkVtIURRlKHgho5+6bKaKQBpkGjW0GFicYPseYFHM81uSvD4MvC+TPiiKosRTLiUms4V+S0WCMYaIZaMGkaJkji4iGxqFniMoKyzbcOfTW9jSdpw/vL6ba05v5Ie3zCfg9/GuO1/g1R2HOXPKGCoDfhbMmcCtl5xc6C4ryogkKgRqEaSFCkEeeXXHYb716Mbo8z+v2cf+jh4m11WxatcRAN5oPQrAy9vbVQgUJY5j3b2s2X2U7l6L86ePY1RF4iGsR4VgSKgQ5BGvSPaC0ybw+HpnEbVtJ3YFVegFrJQoq1uP8lc/epYln7yIs6bW9dv342Wb2Xe026kJEPCx/WAnH714BudNr+f3K3bxj//7Rr/2PoEzJo/hq9efTnN9NV99aD3tnT1sPdgJ6O8oXVQI8ohnrsZGMURsg22baJFsRSl1ntiw3/3f1k8IunutfhazR1NdFedNr2fPkW7ASSddFfTz2Lp9HOjo4bWdR3hl+2F2H+7iwddao68bNyrE+NrKHB9NaaBCkEf6hKDva7ds0xfh4Pf1q6WqKOVET69z7Z8xeQyrdzsu0nGjQn2lJN1Aivee34yI8M5zptDR3csZdzzW72Zq6Wcv4eTxNQD4Sr2iTJZQuymPeEIQmxPdsk3UnxkrEGofKOVGj+WUhYz9HVQEfDHF5W38PumXPM4rKh+xDbbpqznsc/+U9FAhyCPenU1NvEUQGbhdUcoNzyKIvVEK9ROCgbWDvee2MUQs02+bkj468uSRbrcQdm3Mhf7Vh9bR1uH4PmN/AOGIzTf/soH/fmoLE2ormNs0mq5eix+952waairy2/ECs37vMVbsOExNhZ+IZdjcdpwXtx4iYhvOnVbPHe+YW+guKkMg2VIZ70Ypdg4tFG8RSGIhiFgGy1YhGC4qBHni3ue386//tw4RmN5QE93+wtZDAJw7bSxXz21k7Z5jA/b1Wjav7TzC0a5eNu7roGFm+QjB3c9u498eWjdg+9nNdaxqPcraPcf4ytvnqBtghGKMibp6Es2hhQJ982aWDYG48+w9tYyJziF47iIlfVQI8sTG/R2MCvn5zl/P462zGnjjjqsI+nzRMDmPa8+cxJ1PbeHeF3YQjticN72eBz52Ia/vPMwN//189MdSLuxsPwHAh94yjXue3w7Av103lw9cOI0fL9vMtx7dSNiyqfT5C9hLZTj84IlN3Pv8dh78+IXMnFDL4c4wwIC6wk9uaOPXL+7g5e2HBgi+JyIPvtpKQ00IANWBoaNfWZ6IWDajq4JcPdcp7TC6MkhVyD9gwcukMVVMaxgFOHdI3h2Q166nzITAGENddZC5TaOj27w7xooy/U5GOo1j+kI6j3b1suWAE/P/6o7DAEwY3WfxTq6rAuDLf1zD9oMnuOb0gaVRptZXsftIF6taj1IV9A8QEiU1+o3liV7LEEizZqrn4wxbdvRx36Bn5aaDRUrENvhF+vl9Q37n7t8Tx3KzkkY6QTf/z90fauHD96wYkA7ixrOn0OTeENVVB7n5vGZmTahhcl1VwnKTy267lFWtRwn4hGkNo6gO6bA2VPQbyxO9lk0wTZs1KgSRWCHwR7eVE5Zt8PvihMAdMDxx1LUXIwvLds5XVdAZfrxr2ltTGfAJC2IKyVx26oRB3y/g93HOSWNz0NPyQV1DeSJimeidUCq8yIieSF+URKhMBz3LNgSSCIFaBCMT7xKuCrk3N1ZfVBCAT4vM5x0VgjwRse0hu4baO8PRyTEvne76vceSvq7U6LVsdrafwOeTftEi3nfhuYjKzV020vEG/OpQfyvXE4j4yCAl92RaoewmEVkrIraItAzS7m4RaXMrkcVunyciL4rIShFZISLnZdKfYqSto5u3fP0JHl/f1i80dDAqg30RMN5SeW+C1Fs0Uw78fkUrL21r50BHD/4Yt5r3XYyqcL6nfUe7C9I/ZXh4qSCqgvFC4FoEKgR5J1OLYA1wI7A8Rbt7gIUJtn8T+FdjzDzgK+7zkqL1cBd7hjhQLTitzz/6D1fMBBw/aP2oUFYXy/x42WZe33k4a++XbfYfc763b7zzDC6YUc8Xr5nNTz/QwpxJTgSRt7BOo4ZGFt7CL881tGxjGxv3ddBj2WoNFIhMS1WuBxLO5Me1Wy4i0xLtAry4wDHAnkz6U4xYw8gq6v1AoL+/NOT3ZdUf7mV63P6Na7P2ntmk1x0Ybpg/BYC/e1v/+gyeqy1ZKm+lOLFiLILzp9fz/JZDXP295dFtSv4pdNTQZ4BHReS/cKyTtyRrKCK3ArcCNDc356d3WWA4QhBLIG6SNFuTxSOhJGY4Yg9aWMT7bjSF98jCikkO95u/vYC1e46yctcR1uw+yqUpIoSU3JBSCETkcWDgKg643RizJMPP/zjwWWPMgyLybuDnwIJEDY0xdwF3AbS0tIyYX36mQhAfLZMti2AkRB/1pBACz1rK9DtW8otleakgnGiwM6fUceaUuhSvUnJJSiEwxiQcmLPEB4FPu49/D/wsh59VEDK9W5UcuYZGQshlOGIPWnzcyymjQjCyiGhyuKKj0OGje4C3uY8vBzYVsC85IZv+665eiyc2tLG57XjG71WsE6wnwhGe3XSQvUe76I5YVASTX6J+d47AKwGqjAxsY/BJ6rlFJX9kNEcgIjcAPwTGAw+LyEpjzNUi0gT8zBizyG13P3Ap0CAircC/GGN+Dvwt8H0RCQDduHMApcSfVg1v/rs65OdEuH98fENNiG0HO7nyu0+z8p+vYkx1MMmrU1OsFsG3H3uTnz+7jekNo9h2sHNQi8Db95Ula3nf+ScNGna4/WAnrYe76OjuZUf7CSzbcPL4Gk6ZWMOM8TVJX6dkn4i7WlwpHjKNGloMLE6wfQ+wKOb5LUle/yxwTiZ9KHZiJ2XnN6fvB/3Re+bT2dNfCL771/O4409reXx9G4t+8AxNdZXcMH8K7zl/6JPnnT19d9HX//g5/uOGM5gTk9itUOw+3AXAkRNOJsppDdVJ246vreCkcdXsOHRi0AykxhgWfn853b0DxW/K2Cqe/cLlWei5ki4Ry0a9ecVFoV1DJY+I0FxfzYavLuSBj12Y9usunz2Rvzqrqd+2KWOr+eI1swHYfaSLV7Yf5mfPbh1Wvx5btz/6eOWuI6xqPTKs98k2XkioZ7Fcc/qkQdu//4KTgMFdXb2WobvX5uwYIf7ytacxu7GWY129mXZZGSK/enGHzusUGSoEOcYzgyuD/rRzDQ3GzAm1vHL7AlZ+5Uqum9c07B9Urxs1tOLLTixAsbiKvLqzXW41t1QuhIo08g15x3r13MZozvrpDaO45JTxIyJ6qtQ4tXG0poouMvRs5Bg7B/7Q8bXOitqAzzfslBPhiE3QL2kNpPnEOx5P31J9d+kk4/PeMxAjxKGAL+sL9JT00WyhxYVaBDkmkqDOarYI+ISIPbyBrMcNzSy2rKbxFk6qlAOelTWoReB+RyF//1DcUMCHbRyftZI/LNvWyeIiQ4Ugx1h27uKlA34ZtmvIW7XrRd7kK5zUsg3Pbz7Ivc9vxxjTb9K6J2LRG3c86VoEvYMM5t6+WIugIugvOhEsF3L5m1CGh7qGckwu734CPqE3A9dQKOBDRPLqIvnVC9u54/+cYvSv7TzMkpVOeO2okJ/O8MB00qly03tC9h+PrOeev0mcvDYSs5I19nWhGGuiOjS041CGj5VDK1kZHioEOeCOP62lOuTn8wtnY5lcWgS+4VsElt3vjvjOp7fwhYWn5nyRzyG3QDn0r63QGba45JTxVAf9zG+uY+2eY7R1dHPxrIZB32/mBGcNwFMbDyRt41kEoYCPGQ01HDzeTkXQh3eoBzp6qFMlyBuWbaKLAZXiQIUgB9zz/HYARwhybhGkvpP/xH2v8sT6Nu54x1xuOa8ZYwyLX9/NlLFV/dodON7DhNrKJO+SHWItj1h31G1XnsLfXzFryO83Y3wNt14yg1++sD1pm96oReDj3284ncfW7Wf6uFGsrj4KOAXUldyyua2Dhd97JppeYm7TmAL3SIlF5whyjJXDVZR+X3pzBC9sOURPxObZTQeBvrtyL5//164/HchP0ZvYwb8rxhWUyR1iwCeD9r1vjkCYNbGWT142E59PaBzjiJ5GDuWezW3HidiGcaNCnDttLAtPT5THUikUahHkGKfmbm70NuD3EbENxphBXTrBuAlhTzxuanHy/HshpPlY5BM7MdtPCDJwSQVTfA9R11DcOg7PNdajk8U5x7ME7r/1Ak6ZWFvg3ijxqEWQYyzbpF2reKh4k5+pBnBvf1+R8P6Tp/403ycbxN59e4vGoC9H/XAIut9vsolzbxCKPw+hNEJPlezgXVsaLVScqBDkGMs2KSNfhos3sKVKde0Nsj3uwOv9KL1+RYUgD8VqYgfd2H5bGbilPIsn2ZqKqGsozjIrtsV0pUz8zYdSXKhrKMdYxuTs4k+3Qpc3yA6wCPy5sQh2tZ/gqY1tHO3qpTLo55HVexERXt2RvD5yJiLkrQ9IZhF420OBOItAhSBvROJuPpTiQoUgy6zdczT6eMuB46zZfYzJc6sGecXwiRZmSXE37Q2y3oAX/6P0BOXRNft4c38H86bWMWVs8qyfg9Fr2Vz13eX93D7xKbW/fO1pzBg/iifWtxH0+3h952Eunz38EoV9rqGBA/ovX9jOT59xEvPFWwSeEPzwyU2885wpw/58pY99R7tZt/co0xtqaK6vjt5k2Encc0pxoEKQZZbGZPVctqENgHedMzUnn+X9qHpTpJnwBv7tBzuBvsRu3sDo5S769tI3AXjn2VP49rvPGlafei2brl6L6+c1cfu1czjU2cPsxtF89aF1/PzZbXz52tP46MUzACfDajaIuoYSCOL3Ht9Ee2eYMVVBJtX1D42tH+WsHdh+6AQ9EYuKgBZOT8bxnggvbztExDJccsp4KhMUmV+56wjX//i56PN3t0zB7xP+b9VejrsryHUhWXGiQpBlPPfKjPGjoi6Jt84cfFHUcEnXpePt99I3eAOmF0RzdvNYln72Et7cf5xP/uY1OrqHH1fvic7pk8cwvrYiKjLx8xLZxLNoElkEtZUBLp7VwPdvnj9gX0XAz5eumc3X/7xB0yKn4KP3vsKLW9ujz9/dMoX/fOeZ/aK0vMp5X772NL728Hr2H+vh6Tf7L/QbO0oX7hUjGU0Wi8hNIrJWRGwRaUnSZqqILBORdW7bT8fsqxeRpSKyyf0/4lMSegOhZZtoMrNcmcNBnzdJmnwQM8ZEB7lI3ByB3329iBNff+2Zkzh98ui0Fqklw04SHRI/L5FN+iaLB34PEWvwdRz+NOdZyp01u48xKuSPZg19YEXrgJQgnuvx7Wc2cc5JY7FsQ9OYPivsY5fMyEoqdiX7ZHpW1gA3AssHaRMBbjPGzAEuAD4pInPcfV8EnjDGzAKecJ+PaOwYIejNcaREdBAbZOD2xregX6KZNi2TvF8VAX9GSdiSFSbP5WRhMDpZPLDfdorJ+ngftpKYcTUhrpwzkYmjK6Lb4ifZwxFHGEIBH35xFjvGBgF4czJK8ZHRmTHGrDfGbEzRZq8x5jX3cQewHpjs7r4OuNd9fC9wfSb9KQbiLYKgX3KWvyed8FHvTtzz6YYtG8udU0hU4zfTBHTJLAI7h6IYGGSy2CkMlPwyV4sgPSKWGXC9DBCCmJxO3qr3WJfbYPWnlcKS1zMjItOA+cBL7qaJxpi97uN9QNLZQxG5VURWiMiKAweSJxgrNFE3jG3oteycrSqGvsnewdIreP2pDrlCELHxxstEg3IokJkQRC0CSWIR5EAIvKihR1bv5UQ40m+fk+Ij+WvVIkgPZ4V8CiGI9K3gDvidWhn9hEAtgqIl5ZkRkcdFZE2Cv+uG8kEiUgM8CHzGGHMsfr9xqrwn/TUaY+4yxrQYY1rGjx8/lI/OK2/u7wCcjJYHj4ejg1Qu6LMIBqnO5e6rDjlxAT0RO7otkZsmFPBllIRthbtWIH7AP39GPQCzG7OfXqDSjfb58bItzPnKo3zg7pcBeHXHYdo7w3SFk38/nmD912Mb+cafN6ggJMEyAy2rsJV4jiDoF3wiWKa/pVWhQlC0pIwaMsYsyPRDRCSIIwL3GWP+ELNrv4hMMsbsFZFJQFumn1VoVrf2rSNY/PpuxuUwSiK6oMy1CF7e1s4n7nuN77z7LC45ZTy9ls1vX94F9LmGWg+fYPmbTvK5RBO3PRGb7YdODLtPq1uPAHDhjHH9tt90zhQuPWU8E0ZnP7vp2SeN5UvXzCYcsfn20jdZtcvpw5KVuwFork++JsJbjK4eIoYAAB1NSURBVPbAilYArp/fxOzG0Vnv40gnkWUVX8zoKTdCSETw+yR6HjxCGp5btORcosVxkP8cWG+M+U7c7j8BH3QffxBYkuv+5ByBBaf1LY7K5QKaeP/2yl2HOXi8h2c2OT/IZzcf5N8fWQ/A5DpnUds7f/ICdz69BSBhBMfY6mBGfbJsJ2RzatzgKyI5EQFwRO5jbzuZv79iFu89vzlqhXnf/CcvOznpa99+5iS++a4z+aqbgbWnV1cZx/P4uv20d4bxixBrfH7hwTcwMZPBsS7KW85rHvA+gwmyUlgyWkcgIjcAPwTGAw+LyEpjzNUi0gT8zBizCLgIeD+wWkRWui/9J2PMI8A3gAdE5CPADuDdmfSnGOi1bJrq+lYS5zJcznvv+Bh9zxr3cgt9/cYzeOfZU3h07T72H+uOvv70poF3vk11VRm5swpdj9bvk74Je+OkPQ4Mcg4qg37e3TI1Kp5atnIgv3xxBwBXzmlkz5Eu/rJ2H+CElB7rjjCmyrl5qAj6ooWErpwzkbecPI7ntxzia9efziWzxjO1Pjcr7JXMyUgIjDGLgcUJtu8BFrmPn6Xv5iy+3SHgikz6UGyEIzZVoT4TOJdCkCx81Fs57A2I55w0llDAx1+d1ZTyPQNp1jhIRiTBpGI+8fskmnJjKLUgNBNpckaF/MyaUMNb3UH+3edO5UdPbuK/Hnuz35yKneT7rgz6aR6n1kAxo7M3WSRi2dgGqoN9+prLyeJgXPio99+z1oeT+tcnznoDM8wkcLbJXbbVdAj4JBq7nmoxWSyagC45iQTVswJi1wkkuwnQaKHiR89QFvHcCrH1cHMZPuqPrix2k8m5n+9ZBFaSUM7BSLfGQTIiVmEtAl+ca2ioQhA/AaokFgLv2ou9TpKlXNdooeJHcw1lkdg4au9GKZeTxfFRQ2GrvwAMyyKImYBON8jjz6v3snTdfh56Yy9hyx5QCzmfBHxCOGLz8V+/yp/X7GPSmPQmqKO1CXSOYACJ7vQ9j2ckTggSXe9qERQ/KgRZ5DE382hFsO/CfyMmnDTbeD+6h97Yy7ypdVGL4L6XdlJXHYxmQh2KEHg/eHsIrqHvP7GJDfs6os+nN4xK+7XZ5oIZ41j+5kFe3uYkSIuPXkpGbaXj6vjaQ+s4qb6as6bW5ayPIw07gWXlWQS2WyJ02cY2NrUd59QE60SCObSKleygQpBFthxwsi9eNaeRFdsPs/j13TkdFOuqnDUKf1q1h2c2HeCmFifd9djqIHc+vXXAquJ0GE7Khdi76H+4fCafWXBK2q/NNhfPGs/Fs5wFhyfCkbQn6yeOruS+j57Pe3/2Ek9uaCsLIVi16wgGmDfIsRpjeGbTwQHfh/e1LvrBM1i2idabiI2Yu3LORJ7fcqhffiKlOFEhyCLhiE1tRYDxtRWcO62exa/v5vzp9Tn7vMYxlcxoGMXWg50cPtHrfH5lgNe/chXGGPYd6+bQ8TB11ekvahtOyoXYCdbKkD8naSSGg7eaOl0umtlA0E2NUA5c59YO2P6Na5O28RYXxk+iT3ULF3V0R7h4VgOnTqzl+vmTmTOpLyT5Q2+ZxjvPmcLoyszWpii5R4Ugi4Qjdt79oZPHVrHVLTgTse3oHbCIMGlMFZPGDM1fPyyLIGaQGOmJxQI+X9KSl+VIt7sW5VOXzey3vWVaPev/bWE0wVwiRERFYISgQpBFCiEEsUQsk3G46rAsghjX0EiPEAn4JaN6DKWG515MdF1XDcHlqBQ3I/tXW2SErfwLQWz4XjgL2U69UNPhWgQjvdxj0O8bNJtrueGJYiFDgpXco0KQRcIRO++ukVghyKZFkO46gu0HO/sVph/pxcmDahH0I5eV5ZTiQYUgBV1hi/tf3tkvR08yntl0kC7Xp+rdQeW6NF/sgH3weM+geXXSwfvBx77vL57bxrx/e4y/+9WrA9p/6BdOymcvXn9CbW4Sy+ULnSPoT6819LUoyshD5whSsHzTAb70h9UsOqOR/37vOYO2FYE9R7oAuGruRJZtbOSjF0/Paf/2d/QJ1PNbDkVryg4XXwLX0ItbD3HkRG802Vgse4920zi6kidvu5T2E+FoltORSrlEDcW68x5duw/bNoQtmzMmj2FT23HmTa1j4ujK6A2B1houbVQIUuBVvDrcmbpYS/2oEJed6qSgrqsO8ZP3DS4c2eAHN8/ncw+sYt7UOmY31rLojEkZvZ83xxC7oMwbNBLdFDbUVHDBjHFUhfxMDo1sEQCnPsFIcQ0d7erlX5as4eOXzky4kGswvMWGAB+LsfRCfh9hy2bOpNE88umL6XVFUS2C0kaFIAXeIJjOD6EQUUPzm8ey7P9dmrX3i6YOsPpPQoOT3tq2+9eu7SlwpFS2CfpHjmvo1R3t/HHlHnoi9pBvOjrdG5zzp9fznvObeWlbO795aWf0XG9ucxZHeplcdXVwaaNCkIJiF4Js4x/EIgAnkZsvJqt4OGKN+JDRWIJ+GZDWu1jxTtFwEuV52WW/+9fzaKqroiLg4zcv7Yzu90JDI2oRlAUZ/YJF5CYRWSsitoi0JGkzVUSWicg6t+2nY/Z9S0Q2iMgbIrJYRIpuXX/PUIVghPtSEyUT6ycEcdFEhQiZzSUBn4wYiyATInEJCePPoXfOI9E5AhWCUibTX/Aa4EZg+SBtIsBtxpg5wAXAJ0VkjrtvKXC6MeZM4E3gSxn2J+t4pnI6QtBj2f0Szo1EEqUX7okTAmNM9I6yFMQvlpE0R+CdouHUjrDjhcDff/2Hd91HNGqoLMi0Qtl6cJaSD9JmL7DXfdwhIuuBycA6Y8xjMU1fBN6VSX+ySU/E4sn1bexqd6KAUg0OWw4cJxyxqRjhg6K3oCwcseno7mXDvg4OnwhH9z+6dh9f/MNqZjSMIhxxCvGUUkRJyO+LhgCnwhhDR0+kYGkUMq0kB33nO94isGyDZZtou1zW1VAKT17nCERkGjAfeCnB7g8Dv8tnfwbjh09s5kfLNkefP7XxAMs2tkWjgmJp6+jmim8/DTCkBG/FyOgq55K45acv9ts+pirI0a5e7n5uG+GIHU07HfQL189PXQJzpGAwvLrjMIc7w4wdNfi5/MnTW/jmXzZyxuQxvOf8Zjbu6+B9FzQzc8LQIniGy1BShccTrVXhTywE4NwMWO4cgS4oK21SCoGIPA40Jth1uzFmSbofJCI1wIPAZ4wxx+L23Y7jQrpvkNffCtwK0NzcnO7HDptDnc5d8McvPZkjJ8Lc//IuWttPJGzrhZZePKuBW87Lfd9yyRmTx/Dtm87iyY1tTBlbxcnja7hwxjiOdvXy9h8+OyAL5fdvns9J4wpXfyDbNNdX8xyH2Hqwk3NSCMEu93pYvfsoX/rDasCxDH/1kfNz3k9wwkcBXtl+eMivXbaxDehb+FhTMXAo+Ocla6IpqjXFRGmTUgiMMQsy/RARCeKIwH3GmD/E7fsQ8HbgCjOIs9MYcxdwF0BLS0vOZ/PCEZvJdVV8YeFsDnc6QpDMFPcGxw9cOG3EJ+ISEd55zhTeec6Uftu79zsWQGw6CSg93/FfndXE/S/vSqt2ca9lmFxXxTvmNfGTp7YAA7+fXPKL57YBcLwnMuTXNo521nx4qbpjawZcOWciS9ftZ8O+Y5wxeQxAxivWleIm564hcSYQfg6sN8Z8J27fQuDzwNuMMYlvtwtEbDSML0Vq5rDl/PhLKXomHu/YuuKFoICF6nNBRbR2ceoBvdeyCfilYN9BR7cjAOmW44zF74PG0X2vi712PffnrvYT0bmxUhN8pT+Zho/eICKtwIXAwyLyqLu9SUQecZtdBLwfuFxEVrp/i9x9PwJqgaXu9jsz6U82CUesaDRMqvKNXlRNKUXPxBMVgriJVH+J+Y697KnpWAQRy6nlW6hB0uvjcOYK4te8xF67oYCPgE+wjYlJMVFa51npT6ZRQ4uBxQm27wEWuY+fBRJeRcaYmYm2FwOxK2ZTFWuJCkEJWwTeABnv+ig133FoCEXsey2nEFChhWA40a7x6z9iI/9CAR8+nxCJiRpSi6C0Kd2Ra4h0hS1W7TrCse5ejDEc745E3QTej6D9eJj2zvCA13o/yFJaYRtPMpErNdeQd2ec3hxBYYWgx/KEYOhKMNj6j5DftQhsE11HoCkmShtNMeHyrUc3cvdz2/rl43/LyeOAvsHuZ89u42fPbuOtMxvYuL+DkN/HmKog6/Y6QVClLASVAV80hLS2MsC0caPw+YSZE2oK3bWs4gneI6v3Ulcd5PLZE5O2jdiGgF+iGVsBth44nvM+grOGoc8iGLpraLAcURUBH37xLAIbEYqmDrWSG1QIXPYedRaOffTi6Wzef5yp9dW8/Uwnk6fPJ1w3r4mqoJ/fvrKLZzcfZHRlgFMn1nKkK8zCuY2MHRViWkPphFHGE/D7eOYLl2EM1FYESnZgqKsOMroywOPr23hxaztr/vXqpG09i2Bsdd+CssMnernw609w0rhqWk6q54wpY7h89oQhLbrrCluEIzZjqpMvVIt1XR3rjrD1wHFmjO8vylsOHOfdd77Aoc4wDTUhxo2qYOP+DmorA3R0R6hNEDIKjhh2RyxaD3fRerhLrYEyQIXApSdiM7dpNF+65rSE+79/83ws2/DbV3YBMGlMFb/+aH7ixYuFcihEXh0K8MqXF/C5361i7Z6jg7bttQyVQR83nD2ZWRNrCPn9PLJmL6/vPMyh4+HogsR5U+v44ycvSrsP5/3743T0RFj62UuYNTHx4rTH1znrAGorAnT0RPjQL15h+ecv69dm64FODnWGqa0IMLdpTNRy7eiOMLoywIcu6l8r46ZzpvCXtfuY3VjL/KljeWT1Pha/vpuq4MgOiVZSo0LgEo7YKV07fjdCxLJNSU8MlzsVAT8VQV/Kus0RyyZQEaAi4Oeck+oBOGOKE3dvjKG9M8xl//UUW4boLupw1wXsP9aTVAhe2HoQgF/8zbl8/sE3OHJiYL0Mb+7gdx+7kDlNo9m4r4Orv+ekBXvjjoGWzrduOotv3XQWAH97yQz+8y8bHPdXiVp/Sh86mrmkm0Lam2BTISht/CIpfe+9g9SIFhHG1VTwrnOmMtxMEINVSrNsGF9bQcu0ei6YMS5h0SDPexQYJI3EYHjzZZpeovTR0cylx7IJBVKbwN6PqZTXDCjO4JdaCOyUvv9QwJdWBJJH7OL6wdYHWLYdDWII+BL31RMSX5LEcqnwLAG/zhGUPHqGXdJNpxwVArUIShpfGhaBEzWUhhBYdtqpomM/MzJIXQTL7rtj97kRPvF4QhKIppoe2jXrBQToYrLSR+cIcJbSr997jBlpRP14P6ZSDhVVnMHzUGeYZzYd4OzmsYxyI2zW7z3Gso1thCM22w52RpOyJaMvZYVNZRqTrlaMYAwmRJZt97luklkEcbUEhm8RqBCUOioEwItbDwF9E32DcePZk7nz6S3MnjQ6191SCoiXTvz9P3+ZUMDHqq9cRVXIz/88vYU/rtwTbddUN3ien4qYlcppCUHMgG4N5hoyMQO1X6KD/to9R/nGnzdwvCfC6zuPOPvddkO9efGym6oQlD4qBPTFZN8wf3LKtrdddSq3XXVqrrukFJiPX3oyV5w2ga89tJ6Xt7dztKuXqpCfrl6LSWMqWfyJi4jYTobadGg71j1o+K0xhlt++iIvbm2PbvvUb15n/d5j/OPVswe0j7UIDneGCVs2bce6eWrjAZ7ZdDDarirop95Npx3y+xgV8jO+tmLA+yVifG0F+4/1qPVbBqgQ0JdOQCeAFY/KoJ8zp9Tx1+dO5eXt7dFrJByxaaipoDHNjJ/eZPKx7sFTRduGqAi8dWYD4YjNy9vbeWBFa0IhiFgmKgSnNjrW6Z6j3dF+VgZ9dPfaPPQPb41aIj6fsOLLV6Z9h//LD5/Pi1sPcZpavyWPCgExQqB3PkocfUnoLPd/6vUmscxyU3CkihzyInz+8epT+eRlTi7Gf1mypp8bKhbb9AnB7Mba6GeELZugX6L1jONvboZSL+PUxlpObcxPtTWlsOjIhwqBkpxQzGQvpL/eJP71qYTAWzIQe7c+WOhpxDYDJoHDETsa/eYVp68I6jWtpEavEpy7PJHSS6msZE6+hMCzCPxx6aCTpcO2YoXA31dMx+ufF05a4df0EEpqVAiA1sNd+EX65WRXFICKmLTUWw4cZ1Xr0SHdMKRb3yCRRVAR8GPZJmFoqGWbqGgMsAhiC86olaukQaYVym4SkbUiYotIS5I2U0VkmYisc9t+OkGb20TEiEhDJv0ZLmv3HE2ZV0YpT7yB9Oa7XuSKbz8NEI3CSev1adY3iFoEca6hZK+1EriGeiI2O9tPqBAoQybTyeI1wI3A/wzSJgLcZox5TURqgVdFZKkxZh04QgFcBezMsC/DJuj3Mb958IVBSnlySmMtp0ysoaYiwNtOmcDZJ9UxZwhRNN5AvGTlbjp6IsyfWocIHO+OMKoiwOlucfj7XnIu/35C4IrI95/YxILTJlBbGaQi4ONQZ5h1e45FX+u1+8zvVgJOttO5k8bwl7X7dA2AkhaZlqpcDwzqUjHG7AX2uo87RGQ9MBlY5zb5Lk4B+yWZ9CUTwhGb5vrqQn28UsSMrgzy2GffNuzX148K0VBTwbKNB1i28cCA/du/cS0Adz+3DaBflM5J45xr8s6nt3Dn01sGvLbOrVfQFLeW4RcfOpdRFQGOdQ/MSKooichr+KiITAPmAy+5z68DdhtjVqXyz4vIrcCtAM3NzVntV3z9VkXJFtWhAMs/fym7D3fR1tFD6+ETfOHB1QPa1Y8K0XLSWM6dVh/ddsVpE3nza9ew/1g3r+86wto9R+nptWkcU0nI72Ph6Y2AY0X806LZ/McjG7huXhNjXddVQ016C8cUJaUQiMjjQGOCXbcbY9K+ixeRGuBB4DPGmGMiUg38E45bKCXGmLuAuwBaWlqy6tBPN+GcogyH6lCAWRNro7UFHl/fxtJ1+/s3MiRMQREK+JhaX83U+mrecVZT0s/wrl+d61KGQ0ohMMYsyPRDRCSIIwL3GWP+4G4+GZgOeNbAFOA1ETnPGLMv088cCkMNCVSUTEgUdZRpAZgKV0RsFQJlGOTcNSTOKP9zYL0x5jvedmPMamBCTLvtQIsx5uCAN8kxKgRKPkk0getEAQ3/GlSLQMmETMNHbxCRVuBC4GERedTd3iQij7jNLgLeD1wuIivdv0UZ9TqL9EQsOnoiaWWGVJRskEwIMrEIvBsZtQiU4ZBp1NBiYHGC7XuARe7jZ4GUV7gxZlomfRkuR91ar+N1Yk3JE/64wIjDnWH2HeuOFoIZDp4QqEWgDIeyTzrnpQ6oqSz7r0LJE7FhnV99aB33Pr8dgJqK4VulsybU0DSmkptapmTaPaUMKfvRz1v6rznXlXzxuStPpSdi88ymgyxZuYeIbfjytafxjnnJo4JSMWN8Dc9/6Yos9lIpJ8p+9NNaBEq+mdM0mp9+wMnIcrzHsQ4+fNF0JtSmV+NAUbJN2Y9+moJaKQTejUd3r03AJxnNDyhKppT96Oe5hlQIlHzi80k0SkjdkkqhKds5gle2t/OL57bx/BancL26hpR8Ewr4iIQtvQlRCk7ZCsE//3ENG/Z1MLY6yNjqIFM16ZySZ0IBHydUCJQioGyFoDMc4bp5TXz/5vmF7opSpnT3OnWQ49cVKEq+KdtbkXBkaEXIFSXbXHtGE1PGVnHV3EQ5HRUlf5StReAIgaaVUArHt999VqG7oChAmVsE6ptVFEUpZyHQYjSKoihAGbqGjDEYA72W0ZBRRVEUykwIvvmXDfzsmW3RRWSjMkjypSiKUiqUlRDMnjSam8+bim0Mk8ZUcct52a19rCiKMhIpKyF4x1lNg9Z9VRRFKUcyrVB2k4isFRFbRFqStJkqIstEZJ3b9tNx+/9eRDa4+76ZSX8URVGUoZOpRbAGuBH4n0HaRIDbjDGviUgt8KqILDXGrBORy4DrgLOMMT0iMmGQ91EURVFyQKalKtcDyCBL5I0xe4G97uMOEVkPTAbWAR8HvmGM6XH3t2XSH0VRFGXo5DV+UkSmAfOBl9xNpwAXi8hLIvK0iJw7yGtvFZEVIrLiwIEDue+soihKmZDSIhCRx4FEyVBuN8YsSfeDRKQGeBD4jDHmWMzn1wMXAOcCD4jIDGPMgArcxpi7gLsAWlpatEK3oihKlkgpBMaYBZl+iIgEcUTgPmPMH2J2tQJ/cAf+l0XEBhoAveVXFEXJEzl3DYkzgfBzYL0x5jtxu/8IXOa2OwUIAQdz3SdFURSlj0zDR28QkVbgQuBhEXnU3d4kIo+4zS4C3g9cLiIr3b9F7r67gRkisgb4LfDBRG4hRVEUJXfISBx3ReQAsGOYL2+gdKwOPZbiRI+lONFjgZOMMePjN45IIcgEEVlhjEm4+G2kocdSnOixFCd6LMnR9JuKoihljgqBoihKmVOOQnBXoTuQRfRYihM9luJEjyUJZTdHoCiKovSnHC0CRVEUJQYVAkVRlDKnJIRARO4WkTZ3YZq37Q4R2Z1gERsi8iUR2SwiG0Xk6pjtC91tm0XkiwU4joS1G0SkXkSWisgm9/9Yd7uIyA/c/r4hImfHvNcH3fabROSDRXQsI+68uH2oFJGXRWSVezz/6m6f7iZN3CwivxORkLu9wn2+2d0/Lea9Eh5ngY/jHhHZFnNe5rnbi/Yai+mHX0ReF5GH3Ocj6pzEkuBY8nNenGLuI/sPuAQ4G1gTs+0O4P8laDsHWAVUANOBLYDf/dsCzMBJdbEKmJPn45gEnO0+rgXedPv7TeCL7vYvAv/pPl4E/BkQnMR9L7nb64Gt7v+x7uOxRXIsI+68uP0ToMZ9HMTJoHsB8ABws7v9TuDj7uNPAHe6j28GfjfYcRbBcdwDvCtB+6K9xmL6+DngN8BD7vMRdU5SHEtezktJWATGmOVAe5rNrwN+a4zpMcZsAzYD57l/m40xW40xYZyUF9flpMNJMMbsNca85j7uALzaDdcB97rN7gWudx9fB/zSOLwI1InIJOBqYKkxpt0YcxhYCizM46EMdizJKNrzAuB+x8fdp0H3zwCXA//rbo8/N945+1/gChERkh9nXhjkOJJRtNcYgIhMAa4FfuY+F0bYOfGIP5YUZPW8lIQQDMKnXLPpbs+dgjMY7Ypp0+puS7a9IEj/2g0TjVPgB2AfMNF9PBKPBUboeXHN9pVAG84PbAtwxBgTSdC3aL/d/UeBcRTB8cQfhzHGOy//7p6X74pIhbut2M/L94DPA7b7fBwj8Jy4xB+LR87PSykLwU+Ak4F5OBXSvl3Y7qSPJK7dADh3dAx+B1dUJDiWEXtejDGWMWYeMAXnjnF2gbs0LOKPQ0ROB76Eczzn4rgVvlDALqaFiLwdaDPGvFrovmTKIMeSl/NSskJgjNnvXvA28FP6TL3dwNSYplPcbcm25xVJXLthv2v24f73SnqOuGMZqeclFmPMEWAZTtbdOhHx6nrE9i3ab3f/GOAQRXQ8Mcex0HXlGeOUjf0FI+O8XAS8Q0S247gMLwe+z8g8JwOORUR+nbfzkq9JkFz/AdPoP1k8KebxZ3F8gABz6T8xtBVnQjLgPp5O36Tk3DwfgwC/BL4Xt/1b9J8s/qb7+Fr6Txi9bPomjLbhTBaNdR/XF8mxjLjz4vZvPFDnPq4CngHeDvye/hOTn3Aff5L+E5MPDHacRXAck2LO2/dwaokX9TUWd1yX0jfBOqLOSYpjyct5KciB5uCLux/HzdCL4xP7CPArYDXwBvCnuAHodhz/7kbgmpjti3CiW7bglOLM93G8Fcft8waw0v1bhOPHfALYBDzunVj3Ivix29/VQEvMe30YZ9JrM/A3RXQsI+68uH04E3jd7fca4Cvu9hnAy+73/Hugwt1e6T7f7O6fkeo4C3wcT7rnZQ3wa/oii4r2Gos7rkvpGzxH1DlJcSx5OS+aYkJRFKXMKdk5AkVRFCU9VAgURVHKHBUCRVGUMkeFQFEUpcxRIVAURSlzVAgURVHKHBUCRVGUMuf/AwtRH+HvDlm7AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "HVxcswcUFk0G"
      },
      "source": [
        "# Train agent\n",
        "start = time.time()\n",
        "train_steps = 20000\n",
        "model.learn(total_timesteps=train_steps)\n",
        "print (train_steps,\"steps took\",time.time()-start,\"seconds.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "TOnt4KOnWPcR"
      },
      "source": [
        "#model.save('./trained_depth4_50000')\n",
        "trained_model = PPO1.load('/content/trained_depth4_30000.zip')"
      ],
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKpdzi1sWPcO"
      },
      "source": [
        "Finally, we specify the trained agent in the format required for the competition."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "6Nag2SyZWPcY"
      },
      "source": [
        "def my_agent(obs, config):\n",
        "    \n",
        "    #Import saved model trained on best heuristic agent\n",
        "    from stable_baselines import PPO1\n",
        "    trained_model = PPO1.load('../input/ppo1-model-for-connectx')\n",
        "    \n",
        "    # Use the trained model to select a column\n",
        "    col, _ = trained_model.predict(np.array(obs['board']).reshape(6,7,1))\n",
        "    \n",
        "    # Check if selected column is valid\n",
        "    is_valid = (obs['board'][int(col)] == 0)\n",
        "    \n",
        "    # If not valid, select random move. \n",
        "    if is_valid:\n",
        "        return int(col)\n",
        "    else:\n",
        "        return random.choice([col for col in range(config.columns) if obs.board[int(col)] == 0])"
      ],
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "3n1tfC3KltqB"
      },
      "source": [
        "#TO DO fix:\n",
        "\n",
        "#with open('submission.py', 'w') as f:\n",
        "#    f.write(my_agent)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Gt-lWxnWltqE",
        "outputId": "f8eb7fa0-6963-4eba-f87f-52b3e184c4be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#from submission import my_agent\n",
        "def mean_reward(rewards):\n",
        "    epsilon = 0.000001\n",
        "    return sum(r[0] for r in rewards) / sum(r[0] + r[1] + epsilon for r in rewards)\n",
        "\n",
        "# Run multiple episodes to estimate agent's performance.\n",
        "print(\"TODO: fix\")#\"My Agent vs Random Agent:\", mean_reward(evaluate(\"connectx\", [my_agent, \"random\"], num_episodes=1)))\n",
        "print(\"TODO: fix\")#\"My Agent vs Negamax Agent:\", mean_reward(evaluate(\"connectx\", [my_agent, \"negamax\"], num_episodes=1)))"
      ],
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TODO: fix\n",
            "TODO: fix\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "id": "wogSuoXhWPch"
      },
      "source": [
        "def get_win_percentages(agent1, agent2, n_rounds=1):\n",
        "    # Use default Connect Four setup\n",
        "    config = {'rows': 6, 'columns': 7, 'inarow': 4}\n",
        "    # Agent 1 goes first (roughly) half the time          \n",
        "    outcomes = evaluate(\"connectx\", [agent1, agent2], config, [], n_rounds//2)\n",
        "    # Agent 2 goes first (roughly) half the time      \n",
        "    outcomes += [[b,a] for [a,b] in evaluate(\"connectx\", [agent2, agent1], config, [], n_rounds-n_rounds//2)]\n",
        "    print(\"Agent 1 Win Percentage:\", np.round(outcomes.count([1,-1])/len(outcomes), 2))\n",
        "    print(\"Agent 2 Win Percentage:\", np.round(outcomes.count([-1,1])/len(outcomes), 2))\n",
        "    print(\"Number of Invalid Plays by Agent 1:\", outcomes.count([None, 0]))\n",
        "    print(\"Number of Invalid Plays by Agent 2:\", outcomes.count([0, None]))"
      ],
      "execution_count": 169,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xz1FNjYZWPch"
      },
      "source": [
        "And, we calculate how it performs on average, against other agents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "id": "Cxyf62zlWPck",
        "outputId": "0bbd4ed4-254d-44e3-c3bb-7c548a65e239",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "start_time = time.time()\n",
        "n_rounds=3\n",
        "get_win_percentages(agent1=\"random\", agent2=pruner,n_rounds=n_rounds)\n",
        "print (\"Total time taken: {} seconds\".format(time.time() - start_time))\n",
        "print (\"Time taken per round: {} seconds\".format((time.time() - start_time)//n_rounds))"
      ],
      "execution_count": 171,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Agent 1 Win Percentage: 0.0\n",
            "Agent 2 Win Percentage: 1.0\n",
            "Number of Invalid Plays by Agent 1: 0\n",
            "Number of Invalid Plays by Agent 2: 0\n",
            "Total time taken: 28.905571699142456 seconds\n",
            "Time taken per round: 9.0 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}