{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "connectx-getting-started.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidabelin/kaggle-files/blob/main/connect4/connectx-getting-started.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btzX9rh2lgQn"
      },
      "source": [
        "# Install kaggle-environments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "HrzsekYblgQo"
      },
      "source": [
        "# 1. Enable Internet in the Kernel (Settings side pane)\n",
        "\n",
        "# 2. Curl cache may need purged if v0.1.6 cannot be found (uncomment if needed). \n",
        "# !curl -X PURGE https://pypi.org/simple/kaggle-environments\n",
        "\n",
        "# ConnectX environment was defined in v0.1.6\n",
        "!pip install 'kaggle-environments>=0.1.6'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1lgpqTSlgQt"
      },
      "source": [
        "# Create ConnectX Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "id": "JXijotpRlgQu",
        "outputId": "cf617ac0-bc10-462d-87ab-c60f6a6959dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from kaggle_environments import evaluate, make, utils\n",
        "\n",
        "env = make(\"connectx\", debug=True)\n",
        "env.render()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading environment football failed: No module named 'gfootball'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hEI0cUnPlgQx"
      },
      "source": [
        "# Create an Agent\n",
        "\n",
        "To create the submission, an agent function should be fully encapsulated (no external dependencies).  \n",
        "\n",
        "When your agent is being evaluated against others, it will not have access to the Kaggle docker image.  Only the following can be imported: Python Standard Library Modules, gym, numpy, scipy, pytorch (1.3.1, cpu only), and more may be added later.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqxzDaIYlgQy"
      },
      "source": [
        "# This agent random chooses a non-empty column.\n",
        "def my_agent(observation, configuration):\n",
        "    from random import choice\n",
        "    return choice([c for c in range(configuration.columns) if observation.board[c] == 0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyEtsyewlgQ1"
      },
      "source": [
        "# Test your Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gr8CU7gplgQ2"
      },
      "source": [
        "env.reset()\n",
        "# Play as the first agent against default \"random\" agent.\n",
        "env.run([my_agent, \"random\"])\n",
        "env.render(mode=\"ipython\", width=500, height=450)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1WtuUg1lgQ5"
      },
      "source": [
        "# Debug/Train your Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnG0WZMclgQ6"
      },
      "source": [
        "# Play as first position against random agent.\n",
        "trainer = env.train([None, \"random\"])\n",
        "\n",
        "observation = trainer.reset()\n",
        "\n",
        "while not env.done:\n",
        "    my_action = my_agent(observation, env.configuration)\n",
        "    print(\"My Action\", my_action)\n",
        "    observation, reward, done, info = trainer.step(my_action)\n",
        "    env.render(mode=\"ipython\", width=100, height=90, header=False, controls=False)\n",
        "env.render()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qy1htPitlgQ9"
      },
      "source": [
        "# Evaluate your Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4EMlKmmSlgQ-"
      },
      "source": [
        "def mean_reward(rewards):\n",
        "    return sum(r[0] for r in rewards) / float(len(rewards))\n",
        "\n",
        "# Run multiple episodes to estimate its performance.\n",
        "print(\"My Agent vs Random Agent:\", mean_reward(evaluate(\"connectx\", [my_agent, \"random\"], num_episodes=10)))\n",
        "print(\"My Agent vs Negamax Agent:\", mean_reward(evaluate(\"connectx\", [my_agent, \"negamax\"], num_episodes=10)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9g7GbIAlgRB"
      },
      "source": [
        "# Play your Agent\n",
        "Click on any column to place a checker there (\"manually select action\")."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6ypJHa0lgRC"
      },
      "source": [
        "# \"None\" represents which agent you'll manually play as (first or second player).\n",
        "env.play([None, \"negamax\"], width=500, height=450)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1FSEjQzlgRF"
      },
      "source": [
        "# Write Submission File\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rd3R7oEclgRG"
      },
      "source": [
        "import inspect\n",
        "import os\n",
        "\n",
        "def write_agent_to_file(function, file):\n",
        "    with open(file, \"a\" if os.path.exists(file) else \"w\") as f:\n",
        "        f.write(inspect.getsource(function))\n",
        "        print(function, \"written to\", file)\n",
        "\n",
        "write_agent_to_file(my_agent, \"submission.py\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCmT7oInlgRJ"
      },
      "source": [
        "# Validate Submission\n",
        "Play your submission against itself.  This is the first episode the competition will run to weed out erroneous agents.\n",
        "\n",
        "Why validate? This roughly verifies that your submission is fully encapsulated and can be run remotely."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tC6CIUnflgRJ"
      },
      "source": [
        "# Note: Stdout replacement is a temporary workaround.\n",
        "import sys\n",
        "out = sys.stdout\n",
        "submission = utils.read_file(\"/content/submission.py\")\n",
        "agent = utils.get_last_callable(submission)\n",
        "sys.stdout = out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjgniqUmpElG"
      },
      "source": [
        "env = make(\"connectx\", debug=True)\n",
        "env.run([agent, agent])\n",
        "print(\"Success!\" if env.state[0].status == env.state[1].status == \"DONE\" else \"Failed...\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9X-DCNJrlgRO"
      },
      "source": [
        "# Submit to Competition\n",
        "\n",
        "1. Commit this kernel.\n",
        "2. View the commited version.\n",
        "3. Go to \"Data\" section and find submission.py file.\n",
        "4. Click \"Submit to Competition\"\n",
        "5. Go to [My Submissions](https://kaggle.com/c/connectx/submissions) to view your score and episodes being played."
      ]
    }
  ]
}