{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model_trainingX.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOCGP5St_gxH"
      },
      "source": [
        "##Imports and definitions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g_OQUa0FZEFg"
      },
      "source": [
        "import time\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HrG_X1LZEFk"
      },
      "source": [
        "# Check version of tensorflow\n",
        "!pip install 'tensorflow==1.15.0'\n",
        "import tensorflow as tf\n",
        "#tf.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKOc6VGWZwXM"
      },
      "source": [
        "!pip install 'kaggle-environments>=0.1.6'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdN3UqGJZEFy"
      },
      "source": [
        "!apt-get update\n",
        "!apt-get install -y cmake libopenmpi-dev python3-dev zlib1g-dev\n",
        "!pip install \"stable-baselines[mpi]==2.9.0\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYiS-Ykh_o9J"
      },
      "source": [
        "###Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sYkfxvIlZEFp"
      },
      "source": [
        "from kaggle_environments import make, evaluate\n",
        "from gym import spaces\n",
        "\n",
        "class ConnectFourGym:\n",
        "    def __init__(self, agent2=\"random\"):\n",
        "        ks_env = make(\"connectx\", debug=True)\n",
        "        self.env = ks_env.train([None, agent2])\n",
        "        self.rows = ks_env.configuration.rows\n",
        "        self.columns = ks_env.configuration.columns\n",
        "        # Learn about spaces here: http://gym.openai.com/docs/#spaces\n",
        "        self.action_space = spaces.Discrete(self.columns)\n",
        "        self.observation_space = spaces.Box(low=0, high=2, \n",
        "                                            shape=(self.rows,self.columns,1), dtype=np.int)\n",
        "        # Tuple corresponding to the min and max possible rewards\n",
        "        self.reward_range = (-10, 1)\n",
        "        # StableBaselines throws error if these are not defined\n",
        "        self.spec = None\n",
        "        self.metadata = None\n",
        "    \n",
        "    def reset(self):\n",
        "        self.obs = self.env.reset()\n",
        "        return np.array(self.obs['board']).reshape(self.rows,self.columns,1)\n",
        "    \n",
        "    def change_reward(self, step_reward, done):\n",
        "        gridsize = self.rows*self.columns\n",
        "        if step_reward == 1: # The agent won the game\n",
        "            return 10/gridsize\n",
        "        elif done: # The opponent won the game\n",
        "            return -20/gridsize\n",
        "        else: # Reward 1/42\n",
        "            return 1/gridsize\n",
        "    \n",
        "    def step(self, action):\n",
        "        # Check if agent's move is valid\n",
        "        is_valid = (self.obs['board'][int(action)] == 0)\n",
        "        if is_valid: # Play the move\n",
        "            self.obs, step_reward, done, _ = self.env.step(int(action))\n",
        "            reward = self.change_reward(step_reward, done)\n",
        "        else: # End the game and penalize agent\n",
        "            reward, done, _ = -10, True, {}\n",
        "        return np.array(self.obs['board']).reshape(self.rows,self.columns,1), reward, done, _\n",
        "\n",
        "#scoreset_X: 20/max_steps, -30/max_steps, 1/max_steps, -1\n",
        "#scoreset_W: 20/max_steps, -10/max_steps, 1/max_steps, -1\n",
        "#scoreset_U: 10/max_steps, -10/max_steps, 1/max_steps, -1\n",
        "#scoreset_Y: 10/max_steps, -20/max_steps, 1/max_steps, -10\n",
        "#scoreset_Z: 2/max_steps, -10/max_steps, 1/max_steps, -1\n",
        "#scoreset_A: 10/42, -100/42, 1/42, -10\n",
        "#scoreset_B: 1/42, -100/42, 1/42, -420/42\n",
        "#scoreset_C: 1/2*42, -210/42, 1/42, -420/42\n",
        "#scoreset_D: -1/42, -300/42, 1/42, -420/42\n",
        "#scoreset_E: -1/42, -300/42, 2/42, -420/42\n",
        "#scoreset_F: -50/42, -300/42, 1/42, -400/42\n",
        "#scoreset_G: -200/42, -300/42, 1/42, -400/42\n",
        "#scoreset_H: -1/42, -200/42, 2/42, -400/42\n",
        "#scoreset_J: 3/42, -42/42, 1/42, -420/42\n",
        "#scoreset_O: 1, -1, 1/42, -10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XysygiqdoQ3"
      },
      "source": [
        "scoresets_df = pd.DataFrame(data=np.zeros((11,7)),columns=[\"set\",\"coW\",\"W\",\"coL\",\"L\",\"S\",\"score\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7p0vGKq_ZEF2"
      },
      "source": [
        "import os\n",
        "from stable_baselines.bench import Monitor \n",
        "from stable_baselines.common.vec_env import DummyVecEnv\n",
        "# Create directory for logging training information\n",
        "log_dir = \"ppo/\"\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "# Create ConnectFour environment\n",
        "env = ConnectFourGym()\n",
        "# Logging progress\n",
        "monitor_env = Monitor(env, log_dir, allow_early_resets=True)\n",
        "# Create a vectorized environment\n",
        "vec_env = DummyVecEnv([lambda: monitor_env])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUy8-rua_5eC"
      },
      "source": [
        "##Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vu8YzvNxATuW"
      },
      "source": [
        "###Build"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZZslvHb-FoH"
      },
      "source": [
        "from stable_baselines import PPO1, A2C  \n",
        "from stable_baselines.a2c.utils import conv, linear, conv_to_fc\n",
        "from stable_baselines.common.policies import CnnPolicy, MlpPolicy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVkGzQbeZEF6"
      },
      "source": [
        "#@ title model_OG\n",
        "# Neural network for predicting action values\n",
        "def modified_cnn_OG(scaled_images, **kwargs):\n",
        "    activ = tf.nn.relu\n",
        "    layer_1 = activ(conv(scaled_images, 'c1', n_filters=32, filter_size=3, stride=1, \n",
        "                         init_scale=np.sqrt(2), **kwargs))\n",
        "    layer_2 = activ(conv(layer_1, 'c2', n_filters=64, filter_size=3, stride=1, \n",
        "                         init_scale=np.sqrt(2), **kwargs))\n",
        "    layer_3 = conv_to_fc(layer_2)\n",
        "    return activ(linear(layer_3, 'fc1', n_hidden=512, init_scale=np.sqrt(2)))  \n",
        "\n",
        "class CustomCnnPolicy_OG(CnnPolicy):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(CustomCnnPolicy_OG, self).__init__(*args, **kwargs, cnn_extractor=modified_cnn_OG)\n",
        "        \n",
        "# Initialize agent\n",
        "model_OG = PPO1(CustomCnnPolicy_OG, vec_env, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7KgH7xmnlK49"
      },
      "source": [
        "# Neural network for predicting action values\n",
        "def modified_cnn_X(scaled_images, **kwargs):\n",
        "    activ = tf.nn.relu\n",
        "    layer_1 = activ(conv(scaled_images, 'c1', n_filters=32, filter_size=3, stride=1, \n",
        "                         init_scale=np.sqrt(2), **kwargs))\n",
        "    layer_2 = activ(conv(scaled_images, 'c2', n_filters=64, filter_size=2, stride=1, \n",
        "                         init_scale=np.sqrt(2), **kwargs))\n",
        "    layer_3 = conv_to_fc(layer_2)\n",
        "    #layer_4 = activ(linear(layer_3, 'fc1', n_hidden=512, init_scale=np.sqrt(2)))\n",
        "    return activ(linear(layer_3, 'fc2', n_hidden=512, init_scale=np.sqrt(2)))  \n",
        "\n",
        "class CustomCnnPolicyX(CnnPolicy):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(CustomCnnPolicyX, self).__init__(*args, **kwargs, cnn_extractor=modified_cnn_X)\n",
        "        \n",
        "# Initialize agent\n",
        "modelX = PPO1(CustomCnnPolicyX, vec_env, verbose=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUWJ9J1gAasd"
      },
      "source": [
        "###Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHGjGxz-5p4s"
      },
      "source": [
        "def agentX(obs, config, model=modelX, debug=False):\n",
        "    # Use the best model to select a column\n",
        "    col, _ = model.predict(np.array(obs['board']).reshape(6,7,1))\n",
        "\n",
        "    # Check if selected column is valid\n",
        "    is_valid = (obs['board'][int(col)] == 0)\n",
        "    # If not valid, select random move. \n",
        "    if is_valid:\n",
        "        return int(col)\n",
        "    else:\n",
        "        return random.choice([col for col in range(config.columns) if obs.board[int(col)] == 0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ivsVs6EWugF"
      },
      "source": [
        "# Training agents\n",
        "from deep_lookahead import debug_agent as deep_agent\n",
        "from quick_pick_submit import my_agent as test_agent\n",
        "from pruner_v7 import my_agent as heuristic\n",
        "\n",
        "agent_order_X = {0:agentX, 1:agentX, 2:agentX,\n",
        "                3:agentX, 4:agentX, 5:agentX,\n",
        "                6:agentX, 7:agentX, 8:agentX, \n",
        "                9:agentX} \n",
        "\n",
        "agent_order_A = {0:\"negamax\", 1:test_agent, 2:heuristic,\n",
        "                3:agentX, 4:test_agent, 5:agentX,\n",
        "                6:heuristic, 7:agentX, 8:test_agent, \n",
        "                9:agentX}\n",
        "\n",
        "agent_order_B = {0:\"negamax\", 1:test_agent, 2:heuristic,\n",
        "                3:heuristic, 4:test_agent, 5:\"negamax\",\n",
        "                6:test_agent, 7:\"random\", 8:heuristic}\n",
        "\n",
        "agent_order_N = {0:\"negamax\", 1:\"random\", 2:\"negamax\"}\n",
        "\n",
        "agent_order_T = {0:heuristic, 1:agentX, 2:test_agent,\n",
        "                3:agentX, 4:test_agent, 5:agentX,\n",
        "                6:test_agent, 7:agentX, 8:test_agent, \n",
        "                9:agentX}\n",
        "\n",
        "agent_order_P = {0:heuristic, 1:agentX, 2:test_agent,\n",
        "                3:agentX, 4:heuristic, 5:agentX,\n",
        "                6:test_agent, 7:agentX, 8:test_agent, \n",
        "                9:agentX, 10:deep_agent, 11:agentX,\n",
        "                12:deep_agent} \n",
        "\n",
        "agent_order_R = {0:heuristic, 1:agentX, 2:test_agent,\n",
        "                3:agentX, 4:deep_agent, 5:agentX,\n",
        "                6:heuristic, 7:agentX, 8:test_agent, \n",
        "                9:agentX, 10:deep_agent, 11:agentX}\n",
        "\n",
        "agent_order_Q = {0:heuristic, 1:agentX, 2:test_agent,\n",
        "                3:agentX, 4:deep_agent, 5:agentX,\n",
        "                6:heuristic, 7:deep_agent, 8:test_agent, \n",
        "                9:deep_agent, 10:heuristic, 11:test_agent}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1v99MNG-BqB-"
      },
      "source": [
        "#modelX.save('model_Y') #<< next next\r\n",
        "del modelX\r\n",
        "modelX =  PPO1.load('highscorer.zip', env=vec_env)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5ozepFUBtEb"
      },
      "source": [
        "# Train agent\n",
        "high_score = -np.inf\n",
        "lookback = -100\n",
        "agent_order = agent_order_Q\n",
        "for _ in range(10):\n",
        "    start = time.time()\n",
        "    for idx in range(len(agent_order)): \n",
        "        nsteps = 8e4 // (len(agent_order))\n",
        "        env = ConnectFourGym(agent2=agent_order[idx])\n",
        "        modelX.learn(total_timesteps=nsteps)\n",
        "        reward_mean = np.mean(monitor_env.episode_rewards[lookback:])\n",
        "        length_mean = np.mean(monitor_env.episode_lengths[lookback:])\n",
        "        if reward_mean > high_score:\n",
        "            high_score = reward_mean\n",
        "            print(\"New avg high score: {:.5f} \\tAvg Game steps: {:.2f}\".format(reward_mean, length_mean))\n",
        "            modelX.save(\"highscorer\")\n",
        "    stop = time.time()\n",
        "  \n",
        "    # Plot cumulative rewards and lengths\n",
        "    print (nsteps,\"steps took\", (stop-start)// 60, \"minutes and\", round((stop-start) % 60, 2), \"seconds.\")   \n",
        "    print(\"Last mean rewards: {:.5f} \\tLast mean steps:: {:.2f}\".format(reward_mean, length_mean))\n",
        "    with open(os.path.join(log_dir, \"monitor.csv\"), 'rt') as fh:    \n",
        "        firstline = fh.readline()\n",
        "        assert firstline[0] == '#'\n",
        "        df = pd.read_csv(fh, index_col=None)\n",
        "    (df['l'].rolling(window=1024).mean()/21).plot()\n",
        "    df['r'].rolling(window=1024).mean().plot()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_15f5JoC0Gw"
      },
      "source": [
        "from google.colab import files\r\n",
        "modelX.save('model_Y') # <-- next save\r\n",
        "files.download(\"model_Y.zip\")\r\n",
        "files.download(\"highscorer.zip\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGkN478mUef2"
      },
      "source": [
        "#monitor_env.episode_lengths[-1], monitor_env.episode_rewards[-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvgsEOWagkbo"
      },
      "source": [
        "#(df['l'][-7680:].rolling(window=512).mean() - 6).plot()\n",
        "df['r'][-7680:].rolling(window=512).mean().plot()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gq15cfr5ABPi"
      },
      "source": [
        "##Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcFPs_8NZEGj"
      },
      "source": [
        "def get_win_percentages(agent1, agent2, n_rounds=1):\n",
        "    # Use default Connect Four setup\n",
        "    config = {'rows': 6, 'columns': 7, 'inarow': 4}\n",
        "    # Agent 1 goes first (roughly) half the time          \n",
        "    outcomes = evaluate(\"connectx\", [agent1, agent2], config, [], n_rounds//2)\n",
        "    # Agent 2 goes first (roughly) half the time      \n",
        "    outcomes += [[b,a] for [a,b] in evaluate(\"connectx\", [agent2, agent1], config, [], n_rounds-n_rounds//2)]\n",
        "    \n",
        "    print(\"Agent 1 Win Percentage:\", np.round(outcomes.count([1,-1])/len(outcomes), 4))\n",
        "    print(\"Agent 2 Win Percentage:\", np.round(outcomes.count([-1,1])/len(outcomes), 4))\n",
        "    print(\"Number of Invalid Plays by Agent 1:\", outcomes.count([None, 0]))\n",
        "    print(\"Number of Invalid Plays by Agent 2:\", outcomes.count([0, None]))\n",
        "    #return outcomes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56A0wuYqZEGb"
      },
      "source": [
        "def trained_agent(obs, config, model=modelX, debug=True):\n",
        "    # Use the best model to select a column\n",
        "    col, _ = model.predict(np.array(obs['board']).reshape(6,7,1))\n",
        "    if debug:\n",
        "        print(\"Player {} predicts: {}\".format(obs.mark, col))\n",
        "    # Check if selected column is valid\n",
        "    is_valid = (obs['board'][int(col)] == 0)\n",
        "    # If not valid, select random move. \n",
        "    if is_valid:\n",
        "        return int(col)\n",
        "    else:\n",
        "        if debug:\n",
        "            print(\">>> Player {} guesses randomly: {}\".format(obs.mark, col))\n",
        "        return random.choice([col for col in range(config.columns) if obs.board[int(col)] == 0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYC5qkeOesyN"
      },
      "source": [
        "del modelX\r\n",
        "modelX =  PPO1.load('highscorer.zip', env=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAmYNMaFggKm"
      },
      "source": [
        "board = np.array([0, 0, 0, 0, 0, 0, 0,\n",
        "                  0, 0, 0, 0, 0, 0, 0,\n",
        "                  0, 0, 0, 0, 0, 0, 0,\n",
        "                  0, 0, 0, 2, 0, 1, 0,\n",
        "                  0, 0, 0, 2, 0, 1, 0,\n",
        "                  0, 0, 0, 2, 0, 1, 0]).reshape((6,7,1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8rrnzn7XjTy"
      },
      "source": [
        "modelX.action_probability(board)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPNzjQYBh0ND"
      },
      "source": [
        "modelX.predict(board)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVBc94kxZEGn"
      },
      "source": [
        "start_time = time.time()\n",
        "n_rounds=13\n",
        "agent1 = lambda x,y: trained_agent(x,y,model=modelX,debug=False)\n",
        "agent2 = lambda x,y: deep_agent(x,y,debug=False)\n",
        "outcomes = get_win_percentages(agent1, agent2, n_rounds=n_rounds)\n",
        "print (\"Total time taken: {} seconds\".format(time.time() - start_time))\n",
        "print (\"Time taken per round: {} seconds\".format((time.time() - start_time)/n_rounds))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZSvCVDAcHC1"
      },
      "source": [
        "start_time = time.time()\n",
        "n_rounds=11\n",
        "agent1 = agentX \n",
        "agent2 = \"random\"\n",
        "outcomes = get_win_percentages(agent1, agent2, n_rounds=n_rounds)\n",
        "print (\"Total time taken: {} seconds\".format(time.time() - start_time))\n",
        "print (\"Time taken per round: {} seconds\".format((time.time() - start_time)/n_rounds))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkK_5qhVuYwm"
      },
      "source": [
        "outcomes"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}