{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "digit_connect.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "JndnmDMp66FL",
        "266KQvZoMxMv",
        "6sfw3LH0Oycm"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDlWLbfkJtvu",
        "cellView": "form"
      },
      "source": [
        "#@title Everything not mine is copyright 2020 Google LLC. Double-click here for full information.\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        " \n",
        "# Yann LeCun and Corinna Cortes hold the copyright of MNIST dataset,\n",
        "# which is a derivative work from original NIST datasets. \n",
        "# MNIST dataset is made available under the terms of the \n",
        "# Creative Commons Attribution-Share Alike 3.0 license."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9n9_cTveKmse",
        "cellView": "both"
      },
      "source": [
        "# load some standard utilities.\n",
        "#%tensorflow_version 2.x\n",
        " \n",
        "import random as rd\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from matplotlib import pyplot as plt\n",
        "import time\n",
        " \n",
        "print(\"Loaded modules.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEl6htZDbvpO"
      },
      "source": [
        "x_train = pd.read_csv('/content/training_set.csv', delimiter=\",\", index_col=0)\n",
        "#x_train = pd.read_csv('https://github.com/davidabelin/data/blob/master/training_set.csv', delimiter=\",\", index_col=0)            \n",
        "\n",
        "y_train = x_train[\"labels\"]\n",
        "x_train = x_train.drop(labels=\"labels\", axis=1)\n",
        "\n",
        "y_test = y_train.iloc[950:]\n",
        "y_train = y_train.iloc[0:950]\n",
        "\n",
        "x_test = x_train.iloc[950:]\n",
        "x_train = x_train.iloc[0:950]\n",
        "\n",
        "x_test_norm = x_test/2\n",
        "x_train_norm = x_train/2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5wha3y0gVO9"
      },
      "source": [
        "x_train_norm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3014ezH3C7jT"
      },
      "source": [
        "## Create a deep neural net model and a convolutional neural network to compare"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5AI0Drej0KS"
      },
      "source": [
        "# SET UP A DEEP NEURAL NET \n",
        " \n",
        "def create_DNN(learning_rate):\n",
        "    \"\"\"Create and compile a deep neural net.\"\"\"  \n",
        "    # Define the kind of model to use.\n",
        "    model = tf.keras.models.Sequential()\n",
        "    model.add(tf.keras.layers.Input(shape=(42)))\n",
        "    model.add(tf.keras.layers.Dense(units=128, activation='relu'))\n",
        "    model.add(tf.keras.layers.Dropout(rate=0.1)) #avoid overfitting to train set\n",
        "    model.add(tf.keras.layers.Dense(units=256, activation='relu'))\n",
        "    model.add(tf.keras.layers.Dropout(rate=0.2))\n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "    model.add(tf.keras.layers.Dense(units=7, activation='softmax'))     \n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(lr=learning_rate),\n",
        "                    loss=\"sparse_categorical_crossentropy\",\n",
        "                    metrics=['accuracy']) \n",
        "    return model "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1C_tbFXcMHu7"
      },
      "source": [
        " # SET UP A **CONVOLUTIONAL** NEURAL NET \n",
        " \n",
        "def create_CNN(learning_rate):\n",
        "    \"\"\"Create and compile a convolutional neural net.\"\"\"  \n",
        "    # Define the kind of model to use.\n",
        "    model = tf.keras.models.Sequential()\n",
        "    model.add(tf.keras.layers.Conv2D(32, 2, activation='relu', input_shape=(6, 7, 1)))\n",
        "    model.add(tf.keras.layers.Conv2D(64, 2, activation='relu'))\n",
        "    #model.add(tf.keras.layers.Conv2D(128, 2, activation='relu'))\n",
        "    #model.add(tf.keras.layers.MaxPooling2D((2,2)))\n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "    model.add(tf.keras.layers.Dense(units=7, activation='softmax'))     \n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(lr=learning_rate),\n",
        "                    loss=\"sparse_categorical_crossentropy\",\n",
        "                    metrics=['accuracy']) \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQAjVbCtM61V"
      },
      "source": [
        " # train on the training set with 10% held back for validation #\n",
        "def train_model(model, train_features, train_label, epochs,\n",
        "                batch_size=None, validation_split=None):\n",
        "\n",
        "    history = model.fit(x=train_features, y=train_label, \n",
        "                        batch_size=batch_size,\n",
        "                        epochs=epochs, shuffle=True, \n",
        "                        validation_split=validation_split,\n",
        "                        verbose=1)\n",
        "\n",
        "    # Gather the model's metrics after each round of training\n",
        "    epochs = history.epoch\n",
        "    hist = pd.DataFrame(history.history)\n",
        "    return epochs, hist"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "id": "Nn2eBWKXTuaX"
      },
      "source": [
        "# Train and evalate CNN\n",
        "learning_rate = 0.0005\n",
        "epochs = 30\n",
        "batch_size = 10\n",
        "validation_split = None\n",
        "start_time = time.time()\n",
        "\n",
        "dense = create_DNN(learning_rate)\n",
        "#convoluter = create_CNN(learning_rate)\n",
        "\n",
        "# TRAIN X2:\n",
        "for _ in range(2):\n",
        "    #x_train_norm:\n",
        "    epochs_DNN, hist_DNN = train_model(dense, x_train_norm, y_train, epochs, batch_size, validation_split)\n",
        "    batch_time = time.time() - start_time\n",
        "    print(\"Dense finished train set of 950 in {} seconds\".format(round(batch_time,3)))\n",
        "    #epochs_CNN, hist_CNN = train_model(convoluter, x_train_norm, y_train, epochs, batch_size, validation_split)\n",
        "    #batch_time = time.time() - batch_time\n",
        "    #print(\"Convoluter finished train set of 950 in {} seconds\".format(round(batch_time,3)))\n",
        "\n",
        "print (\"\\nTotal time: {:.2} seconds\".format(time.time()-start_time))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7G_xeZOVFICr"
      },
      "source": [
        "dense.evaluate(x=x_test_norm, y=y_test, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LSfMrhSJbIe"
      },
      "source": [
        "x_test_norm.reindex(range(0,len(y_test)-1), method='bfill')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FauE8NVfHuxl"
      },
      "source": [
        "x_test_norm.reindex(list(range(len(y_test))))\n",
        "y_test.reindex(list(range(len(y_test))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMmewY8wIviC"
      },
      "source": [
        "y_test = pd.Series([y for y in y_test])\n",
        "x_test_norm.reindex(y_test.index)\n",
        "x_test_norm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ss4AlURU4Om8"
      },
      "source": [
        "def getKaggles():  \n",
        "    kaggles = pd.DataFrame(columns=['ImageId','Guess','Answer'])  \n",
        "    predicts = dense.predict(x_test_norm)\n",
        "    for j in range(len(x_test_norm)):\n",
        "        probs = predicts[j] # one row of 10 probabilities \n",
        "        max_id = np.argmax(probs)   # index of top probability in row\n",
        "        kaggles.at[j,'ImageId'] = j+1\n",
        "        kaggles.at[j,'Guess'] = max_id\n",
        "        kaggles.at[j,'Answer'] = y_test[j]\n",
        "    return kaggles\n",
        "\n",
        "print(\"Loaded function getKaggles.\")\n",
        "print(\"Getting answers...\" )\n",
        "\n",
        "# LOAD UP ALL THE GUESSES (W/ PROBABILITES) FOR \n",
        "# EACH EXAMPLE IMAGE IN THE NORMALIZED TEST SET\n",
        "kaggles = getKaggles()\n",
        "kaggles"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXDkSujsBtKu"
      },
      "source": [
        "kaggles.to_csv('submission.csv', columns=[\"ImageId\",\"Label\"], index=False) #"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2y_vqIFK4joK"
      },
      "source": [
        "g = pd.read_csv('/content/submission.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOVdPA-L4q5g"
      },
      "source": [
        "s1 = pd.read_csv('/content/submission1.csv', index_col=\"ImageId\")#header=None, "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQHRriJL7vhK"
      },
      "source": [
        "g"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZGqr6PSFZg4"
      },
      "source": [
        "dig = kaggle.iloc[3]\n",
        "dig = np.reshape(list(dig),(28,28))\n",
        "plt.imshow(dig)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9H7W-OKHUnsp"
      },
      "source": [
        "##Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6QCoQqRvRfb"
      },
      "source": [
        "# Plot a graph of the 'accuracy' metric vs. epochs:\n",
        "plt.plot(range(epochs),hist_DNN[\"accuracy\"])\n",
        "plt.plot(range(epochs),hist_DNN[\"val_accuracy\"])\n",
        "plt.plot(range(epochs),hist_CNN[\"accuracy\"])\n",
        "plt.plot(range(epochs),hist_CNN[\"val_accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}